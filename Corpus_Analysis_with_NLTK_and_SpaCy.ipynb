{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPdxy9Y6IqumabTKQqiUVk+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Corpus_Analysis_with_NLTK_and_SpaCy/blob/main/Corpus_Analysis_with_NLTK_and_SpaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus Analysis with NLTK and SpaCy\n",
        "##Introduction\n",
        "\n",
        "In this tutorial, you will learn how to conduct cleaning and text analysis on a corpus of texts using the Natural Language Toolkit (NLTK) and SpaCy. \n",
        "\n",
        "The Natural Language Toolkit (NLTK) is a Python platform dedicated to performing natural language processing, or the computational manipulation of language. Through the NLTK, a suite of libraries can be accessed for the purposes of data mining and analysis, including tokenization, stemming, tagging, parsing, and classification ([NLTK documentation](https://www.nltk.org/)).\n",
        "\n",
        "SpaCy is another popular open-source tool for natural language processing. It's particularly good at annotating linguistic data through part-of-speech tagging, chunking and named entity recognition, as well as calculating document similarity through word embeddings. Whereas NLTK was built for research purposes, SpaCy was built for production, and has an integrated catalogue of features taht is smaller and more streamlined ([SpaCy 101](https://spacy.io/usage/spacy-101#pipelines))\n",
        "\n",
        "By the end of this tutorial, you will be able to: \n",
        "*   Upload a corpus of 2+ texts to Google Colab\n",
        "*   Clean corpora by lowercasing, removing stop words and removing punctuation \n",
        "*   Enrich corpora through stemming, lemmatization, chunking,  part-of-speech tagging, and named entity recognition. \n",
        "*   Perform basic analysis on enriched text including frequency and collocation analysis, concordancing, and indexing \n",
        "*   Visualize results of text analysis through frequency and dispersion plots\n",
        "\n",
        "Table of Contents: \n",
        "1. Install Packages \n",
        "2. Load Text Files into DataFrame\n",
        "2. Cleaning and Tokenization\n",
        "3. Part of Speech Tagging and Parsing\n",
        "4. Word Frequency and Context Analysis"
      ],
      "metadata": {
        "id": "4wnznsMOACSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Install Packages"
      ],
      "metadata": {
        "id": "wVeD4Ik7D43F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrm0CzvO_Uhw"
      },
      "outputs": [],
      "source": [
        "#Imports the Natural Language Toolkit, which is necessary to install NLTK packages and libraries\n",
        "#!pip install nltk\n",
        "import nltk\n",
        "\n",
        "#Installs libraries and packages to tokenize text\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "#Installs libraries and packages to clean text\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "#Installs libraries and packages to stem and lemmatize texts\n",
        "from nltk.stem.snowball import SnowballStemmer # This is \"Porter 2\" and is considered the optimal stemmer.\n",
        "from nltk.stem import (PorterStemmer, LancasterStemmer)\n",
        "nltk.download('wordnet')\n",
        "from nltk import WordNetLemmatizer\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "#Installs libraries and packages to perform chunking, parsing and visualization\n",
        "from nltk import pos_tag\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')\n",
        "!pip install svgling\n",
        "\n",
        "#Imports spaCy itself, necessary to use features (note how few packages are needed for spaCy analysis vs. NLTK above)\n",
        "#!pip install spaCy\n",
        "import spacy\n",
        "#Load the natural language processing pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#Load spaCy visualizer\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Load Text Files into DataFrame\n",
        "\n"
      ],
      "metadata": {
        "id": "zQ8ve667EvxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Oj5Ufz8xE7qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Selet multiple files to upload from local folder\n",
        "from google.colab import files\n",
        "\n",
        "uploaded_files = files.upload()\n"
      ],
      "metadata": {
        "id": "XaVUPnFIE_kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add files into dataframe\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame.from_dict(uploaded_files, orient='index')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "s2w09XuhKqOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reset index and add column names to make wrangling easier\n",
        "df = df.reset_index()\n",
        "df.columns = [\"Title\", \"Text\"]\n",
        "df"
      ],
      "metadata": {
        "id": "BJJPgl5FL9qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Cleaning and Tokenization"
      ],
      "metadata": {
        "id": "Il5slp5CMKeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Basic cleaning with nltk\n",
        "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
        "df['Text'] = df['Text'].apply(lambda x: x.decode('utf-8'))\n",
        "df.head()\n",
        "\n",
        "#Remove newline characters\n",
        "df['Text'] = df['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
        "df['Text'] = df['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
        "df.head()"
      ],
      "metadata": {
        "id": "buepJxsC-wRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lowercase all words\n",
        "df['Text'] = df['Text'].str.lower()\n",
        "\n",
        "#Remove stopwords\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "df['no_stops'] = df['Text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
        "\n",
        "#Remove punctuation\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "s8iYmAYsENde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize with spaCy\n",
        "token_list = []\n",
        "\n",
        "# Disable POS, Dependency Parser, and NER since all we want is tokenizer \n",
        "with nlp.disable_pipes('tagger', 'parser', 'ner'):\n",
        "  #Iterate through each doc object (each text in dataframe) and tokenize, append tokens to list\n",
        "    for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "        word_list = []\n",
        "        for token in doc:\n",
        "            word_list.append(token.text)\n",
        "\n",
        "        token_list.append(word_list)\n",
        "#Make token list a new column in dataframe\n",
        "df['token_list'] = token_list"
      ],
      "metadata": {
        "id": "av2FqJD5HAE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "giWTbWngHPGL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Part of Speech Tagging and Parsing"
      ],
      "metadata": {
        "id": "PcJjGxNrHIFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get lemmas\n",
        "lemma_list = []\n",
        "\n",
        "with nlp.disable_pipes('tagger', 'parser', 'ner'):\n",
        "    for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "        word_list = []\n",
        "        for token in doc:\n",
        "            word_list.append(token.lemma_)\n",
        "\n",
        "        token_list.append(word_list)\n",
        "\n",
        "df['lemma_list'] = lemma_list"
      ],
      "metadata": {
        "id": "X7_NQzt-OXJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare to lemmas retrieved through NLTK "
      ],
      "metadata": {
        "id": "SpEmJjg1HpoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define three stemming tools\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer(\"english\")"
      ],
      "metadata": {
        "id": "jOtbdX3hHpHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Porter'] = df['Tokens'].apply(lambda x: [porter.stem(y) for y in x])\n",
        "df['Lancaster'] = df['Tokens'].apply(lambda x: [lancaster.stem(y) for y in x])\n",
        "df['Snowball'] = df['Tokens'].apply(lambda x: [snowball.stem(y) for y in x])\n",
        "df.head()"
      ],
      "metadata": {
        "id": "QGUgPeiXIPi8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get POS tags with spaCy"
      ],
      "metadata": {
        "id": "v6Fm9k6kX6hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Part of Speech Tags\n",
        "%%timeit\n",
        "pos_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.pos_)\n",
        "        \n",
        "    pos_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in dataframe\n",
        "df['pos_list'] = pos_list"
      ],
      "metadata": {
        "id": "_kBAmxZK_0Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define spaCy part of speech tags"
      ],
      "metadata": {
        "id": "L08vEM3lX-wT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare to NLTK pos tags"
      ],
      "metadata": {
        "id": "_NuqCk_kX9CV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get named entities with spaCy"
      ],
      "metadata": {
        "id": "Pj89QG4bYA5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Named Entities\n",
        "ent_list = []\n",
        "\n",
        "with nlp.disable_pipes('tagger', 'parser'):\n",
        "    for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "        ent_list.append(doc.ents)\n",
        "\n",
        "df['ent_list'] = ent_list"
      ],
      "metadata": {
        "id": "3tlTiFcfJMbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Check dataframe\n",
        "df.head()"
      ],
      "metadata": {
        "id": "50yGYViSLo9T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Frequency and Context Analysis"
      ],
      "metadata": {
        "id": "QFQgly3nNigu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new dataframe for frequency analysis\n",
        "Freqs = corpus[['Title','Word_Tokens', 'pos_list']].copy()\n",
        "Freqs\n",
        "\n",
        "#Get length of words in each text and append to dataframe\n",
        "Freqs['Length'] = Freqs['Word_Tokens'].apply(lambda x: len(x))\n",
        "Freqs"
      ],
      "metadata": {
        "id": "Bft44rVDNpFT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ih1I1y7vNpKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nWOMBVcuNiow"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/yuibi/spacy_tutorial/blob/master/02_intermediate_spacy.ipynb \n",
        "\n",
        "https://www.oreilly.com/library/view/blueprints-for-text/9781492074076/ch04.html \n",
        "\n",
        "https://spacy.io/usage/processing-pipelines "
      ],
      "metadata": {
        "id": "cfh4F4Wy9W8b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "token_list = []\n",
        "\n",
        "# Disable POS, Dependency Parser, and NER since all we want is tokenizer \n",
        "# Alternatively, you can use nlp.make_doc method, which skips all pipelines, if you just need a tokenizer.\n",
        "with nlp.disable_pipes('tagger', 'parser', 'ner'):\n",
        "    for doc in nlp.pipe(texts.Text.astype('unicode').values, batch_size=100):\n",
        "        word_list = []\n",
        "        for token in doc:\n",
        "            word_list.append(token.text)\n",
        "\n",
        "        token_list.append(word_list)\n",
        "\n",
        "texts['token_list2'] = token_list"
      ],
      "metadata": {
        "id": "JudD0k2R9EOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts.head()"
      ],
      "metadata": {
        "id": "NGoC8aUt_r1y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#THIS WORKS\n",
        "\n",
        "token_list = []\n",
        "\n",
        "for doc in nlp.pipe(texts.Text.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.text)\n",
        "        \n",
        "    token_list.append(word_list)\n",
        "\n",
        "texts['token_list'] = token_list"
      ],
      "metadata": {
        "id": "P1RXzVYD8shW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts.head()"
      ],
      "metadata": {
        "id": "pfnchRFJ-opk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "ner_list = []\n",
        "\n",
        "for doc in nlp.pipe(texts.Text.astype('unicode').values, batch_size=100):\n",
        "    ner_list = []\n",
        "    for ent in doc.ents:\n",
        "        ner_list.append(ent.label_)\n",
        "        \n",
        "    ner_list.append(word_list)\n",
        "\n",
        "texts['ner_list'] = pos_list"
      ],
      "metadata": {
        "id": "h9HGEsaPDYSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "id": "rxNZAa-BDQaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%timeit\n",
        "pos_list = []\n",
        "\n",
        "for doc in nlp.pipe(texts.Text.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.pos_)\n",
        "        \n",
        "    pos_list.append(word_list)\n",
        "\n",
        "texts['pos_list'] = pos_list"
      ],
      "metadata": {
        "id": "-QkqPER4DAyD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DixhphWL_zrm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ok6nCT2x-Gl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in nlp.pipe(docs):\n",
        "    tokens = [token.lemma_ for token in doc if token_filter(token)]\n",
        "    filtered_tokens.append(tokens)"
      ],
      "metadata": {
        "id": "tecxjtWc8a3b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "MG_nc2Fq2f3s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = texts['Text'].tolist()\n",
        "\n"
      ],
      "metadata": {
        "id": "9ZWpsBLp4rNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in nlp.pipe(docs):\n",
        "    tokens = [token.lemma_ for token in doc]"
      ],
      "metadata": {
        "id": "vLCextIY42Zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs = df['text'].tolist()\n",
        "\n",
        "def token_filter(token):\n",
        "    return not (token.is_punct | token.is_space | token.is_stop | len(token.text) <= 4)\n",
        "\n",
        "filtered_tokens = []\n",
        "for doc in nlp.pipe(docs):\n",
        "    tokens = [token.lemma_ for token in doc if token_filter(token)]\n",
        "    filtered_tokens.append(tokens)"
      ],
      "metadata": {
        "id": "ntfaFSB04qLd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = texts.apply(lambda row: nlp(row['Text']), axis=1)\n"
      ],
      "metadata": {
        "id": "g7A0hUpn4Iu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "u2SbAw2f3dW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "texts[\"Tag\"] = texts[\"Text\"].apply(lambda my_root: [tok.tag_ for tok in nlp(my_root).tokens])"
      ],
      "metadata": {
        "id": "408vAt8z3P-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = texts['Text'].apply(lambda x: nlp(x))"
      ],
      "metadata": {
        "id": "b7kJlYciMM5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Named Entities\n",
        "ent_list = []\n",
        "\n",
        "with nlp.disable_pipes('tagger', 'parser'):\n",
        "    for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "        word_list = []\n",
        "        for token in doc:\n",
        "          for ent in doc.ents:\n",
        "            word_list.append(ent.text + ent.label_ )\n",
        "\n",
        "df['ent_list2'] = ent_list"
      ],
      "metadata": {
        "id": "FaOWY930ODZ2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}