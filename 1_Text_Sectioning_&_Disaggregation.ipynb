{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Corpus-Analysis-with-SpaCy/blob/main/1_Text_Sectioning_%26_Disaggregation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sOwRC3vIdVf"
      },
      "source": [
        "# Text Sectioning and Disaggregation\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Use this code to clean, section, and disaggregate texts and corpora. \n",
        "\n",
        "**Why Perform Text Sectioning?** \n",
        "\n",
        "Dividing texts into sections (for example, chapters or chunks of N length) is valuable as a precursor to topic modeling and other forms of computational analysis which perform more accurately when applied to groups of segmented documents from longer texts. \n",
        "\n",
        "**Why Disaggregate Texts?** \n",
        "\n",
        "The process of disaggregating the words in texts (in this case, by alphabetizing them) also creates data sets that can be shared freely where original texts cannot be due to copyright restrictions. \n",
        "\n",
        "*Input/Output Specifications:* \n",
        "\n",
        "This code requires plain txt files as input, either those from this repository's sample_data folder or those from a local machine. It returns csv files with disaggregated text grouped by chapter or chunk of n length."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFlH8OZYDYS2"
      },
      "source": [
        "## Upload and Add Text Files To Pandas DataFrame\n",
        "In this section, text files are added into a Pandas DataFrame. Pandas is a fast and relatively easy way to work with large datasets. Though data frames are typically associated with numbers, Pandas also offers many functionalities for [working with textual data. ](https://www.tutorialspoint.com/python_pandas/python_pandas_working_with_text_data.htm) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BxhSLZ6Qg_HY"
      },
      "outputs": [],
      "source": [
        "#Import os and glob\n",
        "import glob\n",
        "import os\n",
        "\n",
        "#Import pandas\n",
        "import pandas as pd\n",
        "\n",
        "#Import nltk for tokenization \n",
        "import nltk\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "nltk.download('punkt')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "koiNZlx3hEkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Selet all files to upload\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "id": "1ZylBR8MhKHM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add files into dataframe\n",
        "import pandas as pd\n",
        "\n",
        "books = pd.DataFrame.from_dict(uploaded, orient='index')\n",
        "books.head()"
      ],
      "metadata": {
        "id": "aNAtH4U2hLyg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reset index and add column names to make wrangling easier\n",
        "books = books.reset_index()\n",
        "books.columns = [\"Title\", \"Text\"]\n",
        "books"
      ],
      "metadata": {
        "id": "9aSLbifdkDaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set dataframes as Colab data tables\n",
        "from google.colab import data_table\n",
        "data_table.enable_dataframe_formatter()"
      ],
      "metadata": {
        "id": "naK0Qc9JhaSx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-74TxYNkiGbi"
      },
      "source": [
        "## Perform Minimal Cleaning and Set Parameters for Sectioning \n",
        "Several basic cleaning processes are implemented: removing unwanted characters from titles and removing encoding  and newline characters from texts. Parameters are then set for part(s) of text to be included in sectioning. In the SciFi Corpus project, \"START OF BOOK\" and \"END OF BOOK\" tags were added to delineate the body of each text. Code in this section removes any text outside the starting and ending parameters--e.g., title page, copyright page, other paratext. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DuqTmZJXg_Hc"
      },
      "outputs": [],
      "source": [
        "books_cleaned = books.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQDxDADog_Hc"
      },
      "outputs": [],
      "source": [
        "#Remove .txt from titles\n",
        "books_cleaned['Title'] = books_cleaned['Title'].str.replace(r'.txt', ' ', regex=True) \n",
        "books_cleaned.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAW72A7fg_Hd"
      },
      "outputs": [],
      "source": [
        "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
        "books_cleaned['Text'] = books_cleaned['Text'].apply(lambda x: x.decode('utf-8', errors=\"ignore\"))\n",
        "\n",
        "#Remove newline characters\n",
        "books_cleaned['Text'] = books_cleaned['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
        "books_cleaned['Text'] = books_cleaned['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
        "books_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KktM2T8g_He"
      },
      "outputs": [],
      "source": [
        "#Split book on start of book tag, keep text only after start of book tag\n",
        "start = books_cleaned[\"Text\"].str.split(\"START OF\", expand = True)\n",
        "books_cleaned['Text'] = start[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIi5SSCag_He"
      },
      "outputs": [],
      "source": [
        "#Split books from project gutenberg on start of ebook label and keep only text after label\n",
        "books_cleaned['Text'] = books_cleaned['Text'].str.replace('THIS PROJECT GUTENBERG EBOOK', '')\n",
        "books_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kys5CJNyswrq"
      },
      "outputs": [],
      "source": [
        "#Split book on end of book tag, keep text only before of book tag\n",
        "end = books_cleaned[\"Text\"].str.split(\"END OF\", expand = True)\n",
        "books_cleaned['Text'] = end[0]\n",
        "books_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ob2j0r6YZsOP"
      },
      "outputs": [],
      "source": [
        "#Check that text is cleaned and sectioned\n",
        "books_cleaned.iloc[0]['Text']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj4jSFppS97y"
      },
      "source": [
        "## Section Texts By Chapter Headings\n",
        "When working with texts with clearly delineated chapters, using chapter headings is a relatively easy way to section texts into segments of (relatively) the same size. After checking the chapter counts for each text to confirm whether sectioning by chapter is a useful procedure, this code iterates through the texts and splits them each time it encounters a new \"chapter\" heading. From here, the text from each chapter is appended to a new dataframe and denoted by book and chapter number. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Smrrk7fOhYwI"
      },
      "outputs": [],
      "source": [
        "#Count number of chapters in each text\n",
        "chapter_counts = books_cleaned['Text'].str.count('CHAPTER')\n",
        "\n",
        "#Append chapter counts to dataframe\n",
        "books_cleaned[\"Chapters\"] = chapter_counts\n",
        "books_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bH_ZGrGjRHV"
      },
      "outputs": [],
      "source": [
        "#Make new cell each time new chapter starts \n",
        "new = books_cleaned[\"Text\"].str.split(\"CHAPTER\", expand = True).set_index(books_cleaned['Title'])\n",
        "new"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pRvTfDVZq7O0"
      },
      "outputs": [],
      "source": [
        "#Flatten dataframe so each chapter is on own row, designated by book and chapter \n",
        "chapters_df = new.stack().reset_index()\n",
        "chapters_df.columns = [\"Book\", \"Chapter\", \"Text\"]\n",
        "chapters_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AtjYB_112gee"
      },
      "outputs": [],
      "source": [
        "#Tidying the DF\n",
        "#Combine book and chapter labels into one column\n",
        "chapters_df['Book + Chapter'] = chapters_df['Book'].astype(str) + '_Chapter_' + chapters_df['Chapter'].astype(str)\n",
        "\n",
        "#Remove individual book and chapter columns\n",
        "chapters_df.drop(columns=['Book', 'Chapter'])\n",
        "\n",
        "#Reindex so book + chapter is first column \n",
        "column_names = \"Book + Chapter\", \"Text\"\n",
        "chapters_df = chapters_df.reindex(columns=column_names)\n",
        "chapters_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-7KVlpthXaJ"
      },
      "source": [
        "## Section Chapters by Chunks of N Length\n",
        "Though chapter headings are useful for splitting texts into semi-equal segments, disparities in chapter length may occur, especially in large corpora. To further segment texts, the text of each text can be divided into chunks of n length. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y1JiAjSDhW94"
      },
      "outputs": [],
      "source": [
        "#Create new df to work with chunks\n",
        "new_chapters_df = chapters_df.copy()\n",
        "\n",
        "#Get number of words in each chapter (helps to determine chunk length)\n",
        "ch_words = new_chapters_df[\"Text\"].apply(lambda x: len(str(x).split(' ')))\n",
        "\n",
        "#Append word counts to dataframe\n",
        "new_chapters_df[\"Word Count\"] = ch_words\n",
        "new_chapters_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-TB1hNp5iDrz"
      },
      "outputs": [],
      "source": [
        "#Tokenize Text\n",
        "new_chapters_df['Tokens'] = new_chapters_df.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)\n",
        "new_chapters_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-8rQnPbRiN2K"
      },
      "outputs": [],
      "source": [
        "#Define chunking function\n",
        "def split(list_a, chunk_size):\n",
        "  for i in range(0, len(list_a), chunk_size):\n",
        "    yield list_a[i:i + chunk_size]\n",
        "\n",
        "#Set desired size of chunks\n",
        "chunk_size = 1000\n",
        "\n",
        "#Create new list for chunked sentences\n",
        "chunked_ch = []\n",
        "\n",
        "#Perform chunking function on each row of tokens\n",
        "s = new_chapters_df['Tokens']\n",
        "for content in s:\n",
        "  chunks = list(split(content, chunk_size))\n",
        "  #Add to new list\n",
        "  chunked_ch.append(chunks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z4xilhl1ibkD"
      },
      "outputs": [],
      "source": [
        "#Create dictionary to associate chunks with titles\n",
        "keys = new_chapters_df['Book + Chapter']\n",
        "values = chunked_ch\n",
        "\n",
        "res = {keys[i]: values[i] for i in range(len(keys))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8TGYS50il1D"
      },
      "outputs": [],
      "source": [
        "#Add chunks to new dataframe\n",
        "chunked_ch_df = pd.DataFrame.from_dict(res, orient='index')\n",
        "chunked_ch_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vDkkG11gipwq"
      },
      "outputs": [],
      "source": [
        "#Reset dataframe index and rename columns\n",
        "chunked_ch_df = chunked_ch_df.stack().reset_index()\n",
        "chunked_ch_df.columns = [\"Title\",\"Chunk\",\"Text\"]\n",
        "chunked_ch_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOwtYw0hi6_k"
      },
      "outputs": [],
      "source": [
        "#Tidying the DF\n",
        "#Combine book and chunk labels into one column\n",
        "chunked_ch_df['Book + Chunk'] = chunked_ch_df['Title'].astype(str) + ' Chunk ' + chunked_ch_df['Chunk'].astype(str)\n",
        "\n",
        "#Remove individual book and chunk columns\n",
        "chunked_ch_df.drop(columns=['Title', 'Chunk'])\n",
        "\n",
        "#Detokenize text\n",
        "TreebankWordDetokenizer().detokenize\n",
        "chunked_ch_df['Text'] = chunked_ch_df.apply(lambda row: TreebankWordDetokenizer().detokenize(row['Text']), axis=1)\n",
        "chunked_ch_df['Text'] \n",
        "\n",
        "#Reindex so book + chunk is first column \n",
        "column_names = \"Book + Chunk\", \"Text\"\n",
        "chunked_ch_df = chunked_ch_df.reindex(columns=column_names)\n",
        "\n",
        "#Print cleaned df\n",
        "chunked_ch_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wS7KWmxq3HQo"
      },
      "source": [
        "## Section Texts By Chunks of N Length\n",
        "When working with texts WITHOUT discernable chapter headings--or, even if chapter headings are present but too infrequent to split texts into meaningful segments--texts can instead be sectioned by chunks of \"N\" length, where N is a variable that can be custom-set below. After checking the word counts for each text to determine what size chunks would be appropriate, this code iterates through the texts and splits them each time it counts \"N\" number of words. From here, the text from each chunk is appended to a new dataframe and denoted by book and chunk number."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWE_NqN-CO4s"
      },
      "outputs": [],
      "source": [
        "#Get number of words in each book (helps to determine chunk length)\n",
        "words = books_cleaned[\"Text\"].apply(lambda x: len(str(x).split(' ')))\n",
        "\n",
        "#Append chapter counts to dataframe\n",
        "books_cleaned[\"Word Count\"] = words\n",
        "books_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XxuTbkEZNKhz"
      },
      "outputs": [],
      "source": [
        "#Tokenize Text\n",
        "books_cleaned['Text'] = books_cleaned['Text'].astype(str)\n",
        "books_cleaned['Tokens'] = books_cleaned.apply(lambda row: nltk.word_tokenize(row['Text']), axis=1)\n",
        "books_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8GLee_W4ZC4G"
      },
      "outputs": [],
      "source": [
        "#Define chunking function\n",
        "def split(list_a, chunk_size):\n",
        "  for i in range(0, len(list_a), chunk_size):\n",
        "    yield list_a[i:i + chunk_size]\n",
        "\n",
        "#Set desired size of chunks\n",
        "chunk_size = 500\n",
        "\n",
        "#Create new list for chunked sentences\n",
        "chunked_sentences = []\n",
        "\n",
        "#Perform chunking function on each row of tokens\n",
        "s = books_cleaned['Tokens']\n",
        "for content in s:\n",
        "  chunks = list(split(content, chunk_size))\n",
        "  #Check that text is being chunked correctly\n",
        "  print(chunks[0])\n",
        "  #Add to new list\n",
        "  chunked_sentences.append(chunks)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcgktYQzb34m"
      },
      "outputs": [],
      "source": [
        "#Create dictionary to associate chunks with titles\n",
        "keys = books_cleaned['Title']\n",
        "values = chunked_sentences\n",
        "\n",
        "res = {keys[i]: values[i] for i in range(len(keys))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "40ruZQhgc3AP"
      },
      "outputs": [],
      "source": [
        "#Add chunks to new dataframe\n",
        "chunked_df = pd.DataFrame.from_dict(res, orient='index')\n",
        "chunked_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CdjZTi9adr2q"
      },
      "outputs": [],
      "source": [
        "#Reset dataframe index and rename columns\n",
        "chunked_df = chunked_df.stack().reset_index()\n",
        "chunked_df.columns = [\"Title\",\"Chunk\",\"Text\"]\n",
        "chunked_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSnpX8s8enM8"
      },
      "outputs": [],
      "source": [
        "#Tidying the DF\n",
        "#Combine book and chunk labels into one column\n",
        "chunked_df['Book + Chunk'] = chunked_df['Title'].astype(str) + ' Chunk ' + chunked_df['Chunk'].astype(str)\n",
        "\n",
        "#Remove individual book and chunk columns\n",
        "chunked_df.drop(columns=['Title', 'Chunk'])\n",
        "\n",
        "#Detokenize text\n",
        "TreebankWordDetokenizer().detokenize\n",
        "chunked_df['Text'] = chunked_df.apply(lambda row: TreebankWordDetokenizer().detokenize(row['Text']), axis=1)\n",
        "chunked_df['Text'] \n",
        "\n",
        "#Reindex so book + chunk is first column \n",
        "column_names = \"Book + Chunk\", \"Text\"\n",
        "chunked_df = chunked_df.reindex(columns=column_names)\n",
        "\n",
        "#Print cleaned df\n",
        "chunked_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htY96uRUg_Hl"
      },
      "source": [
        "## Download CSV Output of Aggregated and Disaggregated Texts \n",
        "\n",
        "At this point, you have three dataframes containing segmented texts that are ready for further analysis. All three (along with the dataframe containing the full texts) can be downloaded as csv files. Depending on the nature of your texts and future analysis, it may be necessary to first disaggregate the data before download. Some analyses like topic modeling work well with \"bag of words\" data, and copyrighted texts cannot be shared in their original forms. Disaggregation, or the breakdown of data into smaller (disordered) parts, is accomplished through the alphabetization of the words in each chapter/chunk.Below, texts are disaggregated and the resulting dataframes can then be downloaded as csvs. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "912vFFtng_Hm"
      },
      "outputs": [],
      "source": [
        "#Download CSVs of aggregated texts\n",
        "\n",
        "#Download CSV with aggregated full texts \n",
        "books_agg = books_cleaned[['Title', 'Text']]\n",
        "books_agg.to_csv('full_texts_agg_output.csv', encoding = 'utf-8-sig')\n",
        "files.download('full_texts_agg_output.csv')\n",
        "\n",
        "#Download CSV with aggregated chapters \n",
        "chapters_df.to_csv('chapters_agg_output.csv', encoding = 'utf-8-sig') \n",
        "files.download('chapters_agg_output.csv')\n",
        "\n",
        "#Download CSV with aggregated chapter chunks \n",
        "chunked_ch_df.to_csv('chapter_chunks_agg_output.csv', encoding = 'utf-8-sig') \n",
        "files.download('chapter_chunks_agg_output.csv')\n",
        "\n",
        "#Download CSV with aggregated chunks\n",
        "chunked_df.to_csv('chunks_agg_output.csv', encoding = 'utf-8-sig') \n",
        "files.download('chunks_agg_output.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jkaef8GKg_Hm"
      },
      "outputs": [],
      "source": [
        "## Disaggregate data in each dataframe\n",
        "\n",
        "#Alphabetize words in each full text\n",
        "books_bow = books_agg.copy()\n",
        "books_bow['Text'] = books_bow['Text'].apply(lambda x: ' '.join(sorted(x.split())))\n",
        "\n",
        "\n",
        "#Alphabetize words in each chapter\n",
        "chapters_df['Text'] = chapters_df['Text'].apply(lambda x: ' '.join(sorted(x.split())))\n",
        "chapters_df\n",
        "\n",
        "\n",
        "#Alphabetize words in each chapter chunk \n",
        "chunked_ch_df['Text'] = chunked_ch_df['Text'].apply(lambda x: ' '.join(sorted(x.split())))\n",
        "\n",
        "#Alphabetize words in each chunk \n",
        "chunked_df['Text'] = chunked_df['Text'].apply(lambda x: ' '.join(sorted(x.split())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8_aXKugTg_Hm"
      },
      "outputs": [],
      "source": [
        "#Download CSVs of disaggregated texts\n",
        "\n",
        "#Download CSV with disaggregated full texts \n",
        "books_bow.to_csv('full_texts_bow_output.csv', encoding = 'utf-8-sig')\n",
        "files.download('full_texts_bow_output.csv')\n",
        "\n",
        "#Download CSV with disaggregated chapters \n",
        "chapters_df.to_csv('chapters_bow_output.csv', encoding = 'utf-8-sig') \n",
        "files.download('chapters_bow_output.csv')\n",
        "\n",
        "#Download disaggregated chapter chunks to csv\n",
        "chunked_ch_df.to_csv('chapter_chunks_bow_output.csv', encoding = 'utf-8-sig') \n",
        "files.download('chapter_chunks_bow_output.csv')\n",
        "\n",
        "#Download disaggregated chunks to csv\n",
        "chunked_df.to_csv('chunks_bow_output.csv', encoding = 'utf-8-sig') \n",
        "files.download('chunks_bow_output.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}