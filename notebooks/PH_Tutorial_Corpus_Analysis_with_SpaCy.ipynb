{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Corpus-Analysis-with-SpaCy/blob/main/notebooks/PH_Tutorial_Corpus_Analysis_with_SpaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus Analysis with spaCy\n",
        "##Introduction\n",
        "\n",
        "This tutorial describes how to conduct cleaning and text analysis on a corpus of texts using spaCy. It will be of interest to researchers who want to prepare a corpus of texts for analysis and perform lemmatization, part-of-speech tagging, and named entity recognition to help answer their research questions. \n",
        "\n",
        "###Why Use spaCy for Corpus Analysis? \n",
        "\n",
        "spaCy is an industrial-strength library for natural language processing. One of its primary usages is to retrieve a variety of linguistic annotations from a text or corpus (e.g. lemmas, part of speech tags, named entities), so it's valuable for researchers who want to know more about their corpora at the lexico-grammatical level. \n",
        "\n",
        "While there are several Python libraries that can conduct similar text-mining tasks, spaCy has the following advantages: \n",
        "*   It's **fast and simple to set up and call the nlp pipeline**; no need to call a wide range of packages and functions for each individual task [(Data Incubator, 2021)](https://www.thedataincubator.com/blog/2016/04/27/nltk-vs-spacy-natural-language-processing-in-python/)\n",
        "*   It uses only the **\"latest and best\" algorithms** for text-processing tasks, so it's easy to run and kept up-to-date by the developers [(Malhotra, 2018)](https://medium.com/@akankshamalhotra24/introduction-to-libraries-of-nlp-in-python-nltk-vs-spacy-42d7b2f128f2)\n",
        "*   It **performs better on text-splitting tasks** than NLTK, since it constructs syntactic trees for each sentence it is called on [(Proxet)](https://proxet.com/blog/spacy-vs-nltk-natural-language-processing-nlp-python-libraries/)\n",
        "\n",
        "###Before You Begin\n",
        "\n",
        "You should have some familiarity with Python or a similar coding platform. For a brief introduction or refresher, work through some of the *Programming Historian's* [introductory Python tutorials](https://programminghistorian.org/en/lessons/introduction-and-installation). You should also have basic knowledge of spreadsheet (.csv) files, as this tutorial will primarily use data in a similar format called a [pandas](https://pandas.pydata.org/) DataFrame. [This lesson](https://programminghistorian.org/en/lessons/crowdsourced-data-normalization-with-pandas) provides an overview to creating and manipulating datasets using pandas.\n",
        "\n",
        "It is also recommended, though not required, that you have some background in methods of computational text mining. [This lesson](https://programminghistorian.org/en/lessons/corpus-analysis-with-antconc) shares tips for working with plain text files and outlines possibilities for exploring keywords and collocates in a corpora (though using a different tool). [This lesson](https://programminghistorian.org/en/lessons/counting-frequencies) describes the process of counting word frequencies, a practice this tutorial will adapt to count part of speech and named entity tags. \n",
        "\n",
        "Two versions of code are provided for this tutorial: one version to be run on Jupyter Notebook and one for Google Colaboratory. Details and setup instructions for each are as follows: \n",
        "*  **Jupyter Notebook** is an environment through which you can run Python on your local machine. Since it's local, it works offline, and you can set up dedicated environments for your projects in which you'll only need to install packages once. If you've used Python before, you likely already have Jupyter Notebook installed on your machine. [This tutorial](https://programminghistorian.org/en/lessons/jupyter-notebooks) covers the basics of setting up Jupyter Notebook using Anaconda.\n",
        "\n",
        "*  **Google Colaboratory** is a Google platform which allows you to run Python in a web browser. Access is free with a Google account and nothing needs to be installed to your local machine. If you're new to coding, aren't working with sensitive data, and aren't running processes with [slow runtime](https://www.techrepublic.com/article/google-colab-vs-jupyter-notebook/), Google Colab may be the best option for you. [Here's a brief Colab tutorial from Google.](https://colab.research.google.com/)\n",
        "\n",
        "\n",
        "###Lesson Dataset: Michigan Corpus of Upper-Level Student Papers (MICUSP)\n",
        "The [Michigan Corpus of Upper-Level Student Papers (MICUSP)](https://elicorpora.info/main) is a corpus of 829 high-scoring academic writing samples from students at the University of Michigan. The papers come from 16 disciplines and seven genres; all were written by senior undergraduate or graduate students and received an A-range score in a university course ([Römer and O'Donnell, 2011](https://web.s.ebscohost.com/ehost/pdfviewer/pdfviewer?vid=0&sid=0b9af0f6-d23e-47ae-90dc-e1ea9fbe606a%40redis); [O'Donnell and Römer, 2012](https://www.euppublishing.com/doi/10.3366/cor.2012.0015)). The papers and their metadata are publically available on MICUSP Simple, an online interface which allows users to search for papers by a range of fields (e.g. genre, discipline, student level, textual features) and conduct simple keyword analyses across disciplines and genres. Metadata from the corpus is available to download in csv form. The text files can be retrieved via webscraping, a process explained further in [this tutorial](https://programminghistorian.org/en/lessons/retired/intro-to-beautiful-soup).\n",
        "\n",
        "Given its size and robust metadata, MICUSP has become a valuable tool for researchers seeking to study student writing computationally. Notably, [Hardy and Römer (2013)](https://web.p.ebscohost.com/ehost/pdfviewer/pdfviewer?vid=0&sid=df763712-4f88-480f-a421-987bb35a09cd%40redis) use MICUSP to study language features that indicate how student writing differs across disciplines, [(Aull, 2019)](https://journals.sagepub.com/doi/epub/10.1177/0741088318819472) compare usages of stance markers across student genres, and [Kim (2018)](https://www.cambridge.org/core/product/identifier/S0266078417000554/type/journal_article) highlights discrepancies between prescriptive grammar rules and actual language use in student work. Though different and framework and approach, these studies are predicated on the fact that computational analysis of *language patterns*--the discrete lexico-grammatical practices students employ in their writing--can yield insights into larger questions about academic writing. Given its value in retrieving *linguistic annotations* like parts of speech and named entities, spaCy is well-poised to conduct this type of analysis using MICUSP.\n",
        "\n",
        "For the purposes of this tutorial, we'll use at a subsection of MICUSP: 67 Biology papers and 98 English papers. Papers in this select corpus belong to all seven MICUSP genres: Argumentative Essay, Creative Writing, Critique/Evaluation, Proposal, Report, Research Paper, and Response Paper. This select corpus and the associated metadata csv are available to download as part of this tutorial's [lesson materials](https://github.com/mkane968/Corpus-Analysis-with-SpaCy/tree/main/lesson-materials). This tutorial will demonstrate how spaCy's utilities in **stopword removal,** **tokenization,** and **lemmatization** can clean and prepare a corpus of student texts for analysis. It will also demonstrate how spaCy's ability to extract linguistic annotations like **part-of-speech tags** and **named entities** can be used to compare conventions within subsets of a discourse community of interest. Here, the focus will be on lexico-grammatical features that may indicate genre and disciplinary differences in academic writing. Finally, this tutorial will address how a dataset enriched by spaCy can be exported in a usable format for further analyses like [sentiment analysis](https://programminghistorian.org/en/lessons/sentiment-analysis#calculate-sentiment-for-a-paragraph) or [topic modeling](https://programminghistorian.org/en/lessons/topic-modeling-and-mallet).\n",
        "\n",
        "###Tutorial Goals\n",
        "By the end of this tutorial, you will be able to: \n",
        "*   Upload a corpus of texts to Google Colab\n",
        "*   Clean the corpus by lowercasing, removing stop words and removing punctuation \n",
        "*   Enrich the corpus through lemmatization, part-of-speech tagging and chunking, and named entity recognition\n",
        "*   Conduct frequency analyses with part-of-speech tags and named entities \n",
        "*   Download an enriched dataset for use in future NLP/analyses\n",
        "\n",
        "###Table of Contents: \n",
        "1. Install and Import Packages \n",
        "2. Load Text Files into DataFrame\n",
        "3. Cleaning and Tokenization\n",
        "4. Text Enrichment\n",
        "    \n",
        "    a. Lemmatization\n",
        "\n",
        "    b. Part of Speech Tagging\n",
        "\n",
        "    c. Named Entity Recognition\n",
        "\n",
        "5. Analysis of Linguistic Annotations\n",
        "\n",
        "    a. Part of Speech Differences Between Disciplines\n",
        "\n",
        "    b. Named Entity Differences Between Genres\n",
        "6. Download Enriched Dataset"
      ],
      "metadata": {
        "id": "4wnznsMOACSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Install and Import Packages\n",
        "\n",
        "Install and import spaCy and related libraries and packages. To improve efficiency, it is common practice to do a single install at the very top of the file instead of interspersing them with your code. These packages can be run in a single cell of code; below, the markdown text describes how each downloaded package or library will be used in the analysis. \n",
        "\n",
        "*Note: the first time you run this code, you may need to install spaCy itself; following this, you can just retrieve the associated packages using the import function*\n"
      ],
      "metadata": {
        "id": "wVeD4Ik7D43F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrm0CzvO_Uhw"
      },
      "outputs": [],
      "source": [
        "#Imports spaCy itself, necessary to use features \n",
        "#!pip install spaCy\n",
        "import spacy\n",
        "#Load the natural language processing pipeline\n",
        "#e'll choose eng_core_web_sm, the small English pipeline which has been tagged on written web texts\n",
        "#!python -m spacy download en_core_web_md \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#Load spaCy visualizer\n",
        "from spacy import displacy\n",
        "#Import drive and files to facilitate file uploads\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "#Import pandas DataFrame packages\n",
        "import pandas as pd\n",
        "#Import graphing package\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Load Text Files into DataFrame\n",
        "\n",
        "After all necessary packages have been installed, it is time to upload the texts for analysis. The key here is to read the texts into Google Colab in a way that will make them recognizable for analysis. Run the following code to “mount” the Google Drive, which allows your Google Colab notebook to access any files on your Drive. A box will pop up asking for permission for the notebook to access your Drive files; click “Connect to Google Drive,” select Google account to connect to, and click “Allow.” "
      ],
      "metadata": {
        "id": "zQ8ve667EvxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Oj5Ufz8xE7qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, load the files for analysis into your Google Drive. To complete this step, you must have the files of interest saved in a folder on your local machine. Once you run this line of code, a button will pop up directing you to “Choose Files” – click the button and a file explorer box will pop up. From here, navigate to the folder where you have stored the papers (.txt files), select the files of interest, and click “Open.” The files will then be uploaded to your Google Drive; you will see the upload complete as output of your cell and can access the files by clicking the file icon in the bar on the left-hand side of the notebook.\n"
      ],
      "metadata": {
        "id": "0FqwCyl8gtNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Selet multiple files to upload from local folder\n",
        "uploaded_files = files.upload()"
      ],
      "metadata": {
        "id": "XaVUPnFIE_kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have files upon which we can perform analysis. To check what form of data we are working with, use the type() function. It should return that your files are contained in a dictionary, where keys are the file names and values are the content of each file. \n"
      ],
      "metadata": {
        "id": "pg0w6jIEgxlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(uploaded_files)"
      ],
      "metadata": {
        "id": "N3f8cxLrgzUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we’ll make the data easier to manage by inserting it into a Pandas DataFrame. This will organize the texts into a table of rows and columns–in this case, the first column will contain the names of the files, and the second column will contain the context of each file. Since the files are currently stored in a dictionary, use the DataFrame.from_dict() function to append them to a new DataFrame.\n"
      ],
      "metadata": {
        "id": "fmYyZTzog32k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add files into DataFrame\n",
        "paper_df = pd.DataFrame.from_dict(uploaded_files, orient='index')\n",
        "paper_df.head()"
      ],
      "metadata": {
        "id": "s2w09XuhKqOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, you can reset the index (the very first row of the DataFrame). This will make data wrangling easier later.  "
      ],
      "metadata": {
        "id": "yr-yzcsng6o4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reset index and add column names to make wrangling easier\n",
        "paper_df = paper_df.reset_index()\n",
        "paper_df.columns = [\"Filename\", \"Text\"]\n",
        "paper_df.head()"
      ],
      "metadata": {
        "id": "BJJPgl5FL9qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will add in metadata of interest to the data frame. Here, we'll add discipline and genre information connected to the MICUSP papers, as we'll be interested in using SpaCy to trace differences across genre and disciplinary categories later. When the cell is run, click \"choose files\", navigate to where you have stored the metadata.csv file, and select this file to puload."
      ],
      "metadata": {
        "id": "IinbZgtMcSOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload csv with essay metadata\n",
        "metadata = files.upload()"
      ],
      "metadata": {
        "id": "ZCASvLyJcq7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll first convert the uploaded csv file to a second DataFrame, drop any empty columns and display the first five rows to check that the data is as expected. Four rows should be present: the paper IDs, their titles, their discipline, and their type."
      ],
      "metadata": {
        "id": "rjYwCN29i51g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_df = pd.read_csv('metadata.csv')\n",
        "metadata_df = metadata_df.dropna(axis=1, how='all')\n",
        "metadata_df.head()"
      ],
      "metadata": {
        "id": "gby6n4lzcq-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the paper ids in this DataFrame are *almost* the same as the paper file names. We're going to make them match exactly so we can merge the two DataFrames together on this column; in effect, linking each text with their title, discipline and genre. \n",
        "\n",
        "To match the columns, we'll remove the \".txt\" tag from the end of each filename in the paper DataFrame, and then we'll rename the paper id column \"Filename.\""
      ],
      "metadata": {
        "id": "99vmOHTKezUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove .txt from titleo f each paper\n",
        "paper_df['Filename'] = paper_df['Filename'] .map(lambda x: x.rstrip('.txt'))\n",
        "\n",
        "#Rename column from paper ID to Title\n",
        "metadata_df.rename(columns={\"PAPER ID\": \"Filename\"}, inplace=True)"
      ],
      "metadata": {
        "id": "RO4lwuwJcrID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is possible to combine the papers and metadata into a single DataFrame. Check the first five rows to make sure each has a filename, title, discipline, paper type and text (the full paper)."
      ],
      "metadata": {
        "id": "TL-Wtoy6gVc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge metadata and papers into new DataFrame\n",
        "#Will only keep rows where both essay and metadata are present\n",
        "final_paper_df = metadata_df.merge(paper_df,on='Filename')\n",
        "\n",
        "#Print DataFrame\n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "2eCYbDExeuqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting DataFrame is now ready for cleaning and analysis. \n"
      ],
      "metadata": {
        "id": "IHOPW8N_g9B_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Cleaning and Tokenization\n",
        "\n",
        "From a quick scan of the DataFrame, it is evident that some preliminary cleaning is required. First use the .decode() module to remove any utf-8 characters embedded in the texts (b'\\xef\\xbb\\xbf). It is also important to remove newline characters (\\n, \\r) through a simple string replacement line. These are NOT functions of spaCy but are necessary to make the code recognizable for further cleaning and tokenization. \n"
      ],
      "metadata": {
        "id": "Il5slp5CMKeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
        "final_paper_df['Text'] = final_paper_df['Text'].apply(lambda x: x.decode('utf-8', errors='ignore'))\n",
        "final_paper_df.head()\n",
        "\n",
        "#Remove newline characters\n",
        "final_paper_df['Text'] = final_paper_df['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
        "final_paper_df['Text'] = final_paper_df['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "buepJxsC-wRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next, most basic operation to perform is lowercasing all tokens in the texts. This will prevent incorrect calculations in later case-sensitive analysis; for example, if lowercasing is not performed, “House” and “house” may be counted as two different words. "
      ],
      "metadata": {
        "id": "xxBqzHdehT7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lowercase all words\n",
        "final_paper_df['Text'] = final_paper_df['Text'].str.lower()\n",
        "\n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "s8iYmAYsENde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to remove punctuation. Depending on your analysis goals, you may want to keep punctuation, but in this case we are interested in words only. "
      ],
      "metadata": {
        "id": "hpu_RZP1IOeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove punctuation and replace with no space (except periods and hyphens)\n",
        "final_paper_df['Text'] = final_paper_df['Text'].str.replace(r'[^\\w\\-\\.\\'\\s]+', '', regex = True)\n",
        "\n",
        "#Remove periods and replace with space (to prevent incorrect compounds)\n",
        "final_paper_df['Text'] = final_paper_df['Text'].str.replace(r'[^\\w\\-\\'\\s]+', ' ', regex = True)\n",
        "\n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "Q0-dpw9yIXY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process used to split up full text into smaller parts for analysis. SpaCy has a built-in function for tokenization that involves segmenting texts into individual parts like words and punctuation. Take the example of an individual sentence: "
      ],
      "metadata": {
        "id": "CIZWFJ5thWZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"This is 'an' example? sentence\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "AYx6IjM_ZiU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this function is doing is calling the spaCy nlp pipeline, which contains the data and components needed for text processing. When the nlp pipeline is called on a sentence, it splits that sentence on each whitespace and reviews its components. Components are then split based on rules for words, punctuations, prefixes, suffixes, etc. Each token is then loaded into a new object that we’ve called “doc.” Calling nlp also enables part of speech tagging, lemmatization, and other enrichment procedures we’ll discuss further below. \n",
        "\n",
        "Since we are working with multiple long texts, we are going to use nlp.pipe, which processes batches of texts as doc objects. Here we’ll tokenize each text in our DataFrame, append each set of tokens to a list, and add the new token lists to a new column in the DataFrame.\n"
      ],
      "metadata": {
        "id": "p5LzNhX6ha97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize with spaCy\n",
        "#Create list for tokens\n",
        "token_list = []\n",
        "\n",
        "# Disable POS, Dependency Parser, and NER since all we want is tokenizer \n",
        "with nlp.disable_pipes('tagger', 'parser', 'ner'):\n",
        "  #Iterate through each doc object (each text in DataFrame) and tokenize, append tokens to list\n",
        "    for doc in nlp.pipe(final_paper_df.Text.astype('unicode').values, batch_size=100):\n",
        "        word_list = []\n",
        "        for token in doc:\n",
        "            word_list.append(token.text)\n",
        "\n",
        "        token_list.append(word_list)\n",
        "        \n",
        "#Make token list a new column in DataFrame\n",
        "final_paper_df['Tokens'] = token_list\n",
        "\n",
        "#Check token list\n",
        "final_paper_df['Tokens']"
      ],
      "metadata": {
        "id": "av2FqJD5HAE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When tokenizing texts, you can also exclude stopwords. Stopwords are words which may hold little significance to text analysis, such as very common words like “the” or “and.” SpaCy has a built-in dictionary of stopwords which you can access. You can also add or remove your own stopwords, as shown below:"
      ],
      "metadata": {
        "id": "FUQF0upEhdm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding and removing stopwords to default list\n",
        "#See list of default stopwords\n",
        "print(nlp.Defaults.stop_words)\n",
        "\n",
        "#Remove a  stopword\n",
        "nlp.Defaults.stop_words.remove(\"becomes\")\n",
        "\n",
        "#Add stopword\n",
        "nlp.Defaults.stop_words.add(\"book\")\n",
        "\n",
        "#Check updated list of default stopwords\n",
        "print(nlp.Defaults.stop_words)"
      ],
      "metadata": {
        "id": "lBveRLWFSASN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tokenize texts without stopwords, follow the same process above using nlp.pipe, but only append tokens to list that are NOT included in stopwords list and append these to a new row in the DataFrame. \n"
      ],
      "metadata": {
        "id": "Il_t3zO-hflr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove all stopwords and append remaining tokens to new df column\n",
        "token_list_nostops = []\n",
        "\n",
        "# Disable POS, Dependency Parser, and NER since all we want is tokenizer \n",
        "with nlp.disable_pipes('tagger', 'parser', 'ner'):\n",
        "  #Iterate through each doc object (each text in DataFrame) and tokenize, append tokens to list\n",
        "    for doc in nlp.pipe(final_paper_df.Text.astype('unicode').values, batch_size=100):\n",
        "        nostops_word_list = []\n",
        "        for token in doc:\n",
        "            if token.text not in nlp.Defaults.stop_words:\n",
        "              nostops_word_list.append(token.text)\n",
        "\n",
        "        token_list_nostops.append(nostops_word_list)\n",
        "\n",
        "#Make token list a new column in DataFrame\n",
        "final_paper_df['Tokens_NoStops'] = token_list_nostops\n",
        "\n",
        "#Check list of tokens without stopwords\n",
        "final_paper_df['Tokens_NoStops']"
      ],
      "metadata": {
        "id": "8zyfrC5FS_bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we have two tokenized lists: one with and without stopwords. Depending on the purposes of your analyses, you might need your data in token form. One of the benefits of spaCy, however, is that it's streamlined: when you ask it to tag parts-of-speech in a dataset, its *first* step (behind the scenes) is to tokenize the data. In other words, we don't need to run this tokenization process before running part-of-speech tagging or other analyses; we're just doing it here to demonstrate how spaCy works under-the-hood, and to get a dataset that's useful for [other types of analysis](https://towardsdatascience.com/tokenization-for-natural-language-processing-a179a891bad4). Many forms of sentiment analysis, for example, require data to be tokenized, as this form of analysis involves finding the polarity of each separate word in a document.\n",
        "\n",
        "Even so, tokenizing the dataset without stopwords will be of value to our analysis. We can do this once, at the outset, to speed up runtime later--in essence, removing the stopwords once, so the nlp pipeline doesn't have to do the same every time. All we need to do to make the dataset with no stopwords useable for spaCy analysis is to transform each essay from a list of tokens back into a string. "
      ],
      "metadata": {
        "id": "eNg78OAZi3Ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_paper_df['Text_NoStops'] = [' '.join(map(str, l)) for l in final_paper_df['Tokens_NoStops']]\n",
        "final_paper_df['Text_NoStops']"
      ],
      "metadata": {
        "id": "_UQntgJEi0rm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why analyze text without stopwords? One case where stopword removal may be useful is if you want to compare document similarity. SpaCy calculates document similarity based on corpus word vectors; since stopwords are words that appear throughout texts, they will heighten document similarity scores even if their content is very different. To make this type of analysis more accurate, we'll load a larger spaCy pipeline with vectors (en_core_web_md). \n"
      ],
      "metadata": {
        "id": "HOGEZ1GUhiZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopwords Test Case - Word Vector Similarity\n",
        "#Load a larger pipeline with vectors to make stopword analysis more accurate\n",
        "!spacy download en_core_web_md\n",
        "nlp2 = spacy.load(\"en_core_web_md\")"
      ],
      "metadata": {
        "id": "gZF8YTdmelu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe the difference between these two texts, with and without stopwords. As expected, these two texts in different disciplines are less similar with stopwords removed."
      ],
      "metadata": {
        "id": "a6BYJH89nwKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare similarity between two documents without stopwords\n",
        "doc1 = nlp2(final_paper_df.Text[0])\n",
        "doc2 = nlp2(final_paper_df.Text[100])\n",
        "print(f'The similarity between ' + str(final_paper_df.Filename[0]) + ' and ' + str(final_paper_df.Filename[100]) + ' with stopwords is ' + str(doc1.similarity(doc2)))\n",
        "\n",
        "# Compare similarity between two documents with stopwords\n",
        "doc1 = nlp2(final_paper_df.Text_NoStops[0])\n",
        "doc2 = nlp2(final_paper_df.Text_NoStops[72])\n",
        "print(f'The similarity between ' + str(final_paper_df.Filename[0]) + ' and ' + str(final_paper_df.Filename[100]) + ' without stopwords is ' + str(doc1.similarity(doc2)))\n"
      ],
      "metadata": {
        "id": "qXUK0X_Wnya7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopword removal is also useful for topic modeling and classification tasks, where finding general themes across documents is the goal. However, other types of analysis like sentiment analysis are highly sensitive and removing stopwords will change sentence meaning (e.g. removing “not” in the sentence “I was not happy”). When possible, it is recommended to run analysis with and without stopwords and see how the model is impacted. For the rest of this tutorial, we will be using the corpus without stopwords, but you are welcome to replicate analysis with them. \n",
        "\n",
        "At this point, we have a cleaned DataFrame on which we can use spaCy for further text enrichment. But if you don't need lemmas, parts of speech, and named entities for your further analytic purposes, you can download the DataFrame as it is to your machine using this code. "
      ],
      "metadata": {
        "id": "M9r1k5BXhtYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_paper_df.to_csv('final_paper_df.csv') \n",
        "files.download('final_paper_df.csv')"
      ],
      "metadata": {
        "id": "9ZQa-iFvlffF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create a new DataFrame to which we'll add the spaCy enrichments (lemmas, part of speech tags, named entities). We'll only keep the filenames, discipline and paper type labels, and the text without stopwords. "
      ],
      "metadata": {
        "id": "5nIs6L_Hlfqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new DataFrame for text enrichment\n",
        "enriched_df = final_paper_df[['Filename','DISCIPLINE','PAPER TYPE','Text_NoStops']].copy()\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "1xYi2qi3ibLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also use the smaller spaCy pipeline which is optimized for tagging, parsing, lemmatizing and ner. It's not as accurate as the en_core_web_md (and _lg) models, but processing will be faster. [Learn more about the models here](https://spacy.io/models)."
      ],
      "metadata": {
        "id": "vUrcYvEqot4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Text Enrichment"
      ],
      "metadata": {
        "id": "PcJjGxNrHIFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Lemmatization\n",
        "\n",
        "SpaCy enables several types of text enrichment. We’ll start with lemmatization, which retrieves the dictionary root word of each word (e.g. “brighten” for “brightening”). Lemmatization is one of the functions that occurs when the nlp pipe is called; repeat the same process as above to iterate through each document in the dataframe and this time append all lemmas to new column. \n"
      ],
      "metadata": {
        "id": "lDBswM5fo6Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get lemmas\n",
        "lemma_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is lemmatization \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag lemma, append lemma to list\n",
        "  for doc in nlp.pipe(enriched_df.Text_NoStops.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.lemma_)\n",
        "        \n",
        "    lemma_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in dataframe\n",
        "enriched_df['Lemmas'] = lemma_list\n",
        "enriched_df['Lemmas'] = [' '.join(map(str, l)) for l in enriched_df['Lemmas']]\n",
        "\n",
        "#Check lemmas\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "X7_NQzt-OXJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization can help reduce noise and refine results for researchers who are conducting keyword searches. For example, let's compare counts of the word \"write\" in the original token column and in the lemmatized column"
      ],
      "metadata": {
        "id": "dwV7mzEOkR5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'\"Write\" appears in the text tokens column ' + str(enriched_df.Text_NoStops.str.count(\"write\").sum()) + ' times.')\n",
        "print(f'\"Write\" appears in the lemmas column ' + str(enriched_df.Lemmas.str.count(\"write\").sum()) + ' times.')"
      ],
      "metadata": {
        "id": "0PA6wMTkqWl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, there are more instances of \"write\" in the lemmas column, as the lemmatization process has grouped inflected word forms (writing, writer) into the base word \"write.\" For similar rasons, lemmatized data is also often used to [create topic models](https://towardsdatascience.com/topic-modelling-in-python-with-spacy-and-gensim-dc8f7748bdbf)."
      ],
      "metadata": {
        "id": "2I5QC-nnssGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Part of Speech Tagging"
      ],
      "metadata": {
        "id": "zBUNn3BMkPQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nlp pipeline also enables the tagging of each word according to its part of speech. This code will append all parts of speech to a new DataFrame column. \n"
      ],
      "metadata": {
        "id": "FG9B5mpFh0-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get part of speech tags\n",
        "pos_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(enriched_df.Text_NoStops.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.pos_)\n",
        "        \n",
        "    pos_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in DataFrame\n",
        "enriched_df['POS_Tags'] = pos_list\n",
        "enriched_df['POS_Tags'] = [' '.join(map(str, l)) for l in enriched_df['POS_Tags']]\n",
        "\n",
        "#Check pos tags\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "_kBAmxZK_0Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, you may want to get only a set of Part of Speech tags for further analysis--all of the proper nouns, for instance. "
      ],
      "metadata": {
        "id": "_90TFpWPuZ36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get specific subset of part of speech tags\n",
        "propnoun_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(enriched_df.Text_NoStops.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "      if token.pos_ == 'PROPN':\n",
        "        word_list.append(token)\n",
        "        \n",
        "    propnoun_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in DataFrame\n",
        "enriched_df['Proper_Nouns'] = propnoun_list\n",
        "enriched_df['Proper_Nouns'] = [', '.join(map(str, l)) for l in enriched_df['Proper_Nouns']]\n",
        "\n",
        "#Check proper noun tags\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "z5mtmwpxuaNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out the dictionary of SpaCy POS tags [here](https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/#:~:text=Spacy%20POS%20Tags%20List,-Every%20token%20is%20assigned%20a) and feel free to test out the process of retreiving different parts of speech using the code above. \n",
        "\n",
        "Closely related to POS tagging is dependency parsing, wherein SpaCy identifies how different segments of a text are related to each other. Once the grammatical structure of each sentence is identified, visualizations can be created to show the connections between different words. Since we are working with large texts, our code will break down each text into sentences (spans) and then create dependency visualizers for each span\n"
      ],
      "metadata": {
        "id": "-XOgbYCduPII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get dependency parsing for a single doc\n",
        "doc = nlp(enriched_df.Text_NoStops[0]) \n",
        "print(doc)\n",
        "\n",
        "#Make each sentence a span to break up dependency visualizations\n",
        "spans = doc.sents\n",
        "\n",
        "#Create dependency visualizations \n",
        "displacy.render(spans, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "63qLzAWYuKiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dependency parsing also enables the extraction of larger chunks of text, like noun phrases; let's try it out:"
      ],
      "metadata": {
        "id": "1cv9_pdruivA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get part of speech tags\n",
        "np_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(enriched_df.Text_NoStops.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for np in doc.noun_chunks:\n",
        "      word_list.append(np)\n",
        "    np_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in DataFrame\n",
        "enriched_df['Text_NounPhrases'] = np_list\n",
        "enriched_df['Text_NounPhrases'] = [', '.join(map(str, l)) for l in enriched_df['Text_NounPhrases']]\n",
        "\n",
        "#Check pos tags\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "RRaCFQc_ui4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part of speech tagging is another way to reduce dimensionality of the data and identify how language is being used at the lexico-grammatical level. Some types of further analysis, like topic modeling, may work when only certain parts of speech, [like nouns,](https://aclanthology.org/U15-1013.pdf) are retained. And as we'll explore further below, parts of speech can be used to compare patterns of language use within subsets of a corpora (e.g. texts of different genres and disciplines)"
      ],
      "metadata": {
        "id": "B1aa-Xg3ySZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c. Named Entity Recognition"
      ],
      "metadata": {
        "id": "5L_tQTFV4HOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, SpaCy can tag “named entities” in your text, such as names, dates, organizations, and locations. We’ll again call the nlp pipeline on each document in the corpus and append the named entity tags to a new column. \n"
      ],
      "metadata": {
        "id": "39utqaRyh_M1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Named Entities\n",
        "ner_list = []\n",
        "\n",
        "with nlp.disable_pipes('tagger', 'parser'):\n",
        "    for doc in nlp.pipe(enriched_df.Text_NoStops.astype('unicode').values, batch_size=100):\n",
        "      ent_list = []\n",
        "      for ent in doc.ents:\n",
        "        ent_list.append(ent.label_)\n",
        "      ner_list.append(ent_list)\n",
        " \n",
        "enriched_df['NER_Tags'] = ner_list\n",
        "enriched_df['NER_Tags'] = [' '.join(map(str, l)) for l in enriched_df['NER_Tags']]\n",
        "\n",
        "\n",
        "#Check named entities\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "3tlTiFcfJMbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can add another column with the words and phrases identified as named entities."
      ],
      "metadata": {
        "id": "jrV6_VSCydsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Named Entitie words\n",
        "ent_w_list = []\n",
        "\n",
        "with nlp.disable_pipes('tagger', 'parser'):\n",
        "    for doc in nlp.pipe(enriched_df.Text_NoStops.astype('unicode').values, batch_size=100):\n",
        "        ent_w_list.append(doc.ents)\n",
        "\n",
        "enriched_df['NER_Words'] = ent_w_list\n",
        "enriched_df['NER_Words'] = [' '.join(map(str, l)) for l in enriched_df['NER_Words']]\n",
        "\n",
        "\n",
        "#Check named entities\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "bO86j0szydz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy also allows you to visualize named entities within single texts, as follows: "
      ],
      "metadata": {
        "id": "fq0WXwUlyiQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get named entities in a single document and visualize\n",
        "doc = nlp(enriched_df.Text_NoStops[0]) \n",
        "\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "ldCTllIVyiWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named entity recognition allows researchers to take a closer look at the \"real-world objects\" that are present in their texts: names, persons, institutions, events, works of art, and [more](https://www.kaggle.com/code/curiousprogrammer/entity-extraction-and-classification-using-spacy). In addition to studying named entities that spaCy automatically recognizees, you can use a training dataset to update the categories or create a new entity category, as in [this example](machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/)."
      ],
      "metadata": {
        "id": "indhranu0rSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Analysis of Linguistic Annotations"
      ],
      "metadata": {
        "id": "XUGnN4BX4C4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why are spaCy's linguistic annotations useful to researchers? Below are two examples of how researchers can use data about the MICUSP corpus, produced through spaCy, to draw conclusions about discipline and genre conventions in student academic writing. "
      ],
      "metadata": {
        "id": "_e12LSYQJSdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Part of Speech Analysis"
      ],
      "metadata": {
        "id": "9QTJrt6byTlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Research Question:** Do students use certain *parts of speech* more frequently in Biology papers versus English papers that signify differences in disciplinary conventions?  \n",
        "\n",
        "Prior research has shown that even when writing in the same genres, writers in the sciences follow different conventions than those in the humanities. Notably, academic writing in the sciences has been characterized informational, descriptive, and procedural, while that in the humanities is narrativized, evaluative, and situation-dependent (i.e. focused on discussing a particular text or prompt) [(Hardy and Römer (2013)](https://web.p.ebscohost.com/ehost/pdfviewer/pdfviewer?vid=0&sid=df763712-4f88-480f-a421-987bb35a09cd%40redis). \n",
        "\n",
        "By deploying spaCy on the MICUSP texts, researchers can determine whether there are any significant differences between the part-of-speech tag frequencies in English and Biology papers. For example, we might expect students writing Biology papers to use more adjectives than those in the humanities, given their focus on description. Conversely, we might suspect English papers have more verbs and verb auxiliaries, indicating a more narrative structure. We'll use spaCy to test these hypotheses as well as explore other part-of-speech count differences that could prompt further investigation. \n",
        "\n",
        "To start, we'll create a new DataFrame with the text filenames, disciplines, and part of speech tags. "
      ],
      "metadata": {
        "id": "QdhhHB6Ol-vQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new DataFrame for analysis purposes\n",
        "pos_analysis_df = enriched_df[['Filename','DISCIPLINE', 'POS_Tags']]\n",
        "pos_analysis_df.head()"
      ],
      "metadata": {
        "id": "r2JKOyTuJtDx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before beginning the analysis, calculate the total number of part-of-speech tags in each discipline. This indicates to what extent our two sample groups are well-balanced. In this case, they're close enough (within a few thousand words) that we'll proceed with comparing the raw average frequency counts of each part of speech per discipline, though in a more thorough analysis it's recommended to normalize any frequency results using a method like term frequency-inverse document frequency [(tf-idf)](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf)."
      ],
      "metadata": {
        "id": "mnHLpvHGPf5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate the frequency of each POS tag as it appears in each cell in the POS_Tags column of the dataframe \n",
        "pos_analysis_df['POS_Counts'] = pos_analysis_df.POS_Tags.apply(lambda x: len(str(x).split(' ')))\n",
        "\n",
        "#Sum the total number of each POS tags as it appears within all cells and add to DataFrame called counts \n",
        "counts = pos_analysis_df.groupby(['DISCIPLINE']).sum()\n",
        "\n",
        "#Print counts\n",
        "counts"
      ],
      "metadata": {
        "id": "po_LIvwJl-5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy's pipeline includes a way to count the number of each part of speech tag that appears in each document (ex. # times NOUN tag appears in a document, # times VERB tag appears, etc). This is called using ```doc.count_by(spacy.attrs.POS)``` Here's how it works on a single sentence. "
      ],
      "metadata": {
        "id": "NQZ6919Oou-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"This is 'an' example? sentence\")\n",
        "\n",
        "print(doc.count_by(spacy.attrs.POS))"
      ],
      "metadata": {
        "id": "T5RAD_t_SJt2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output is a dictionary that lists the unique index of each part of speech and the number of times that part of speech has appeared in the example sentence. To associate the actual parts-of-speech associated with each index, a new dictionary can be created which replaces the index of each part of speech for its label. In the example below, it's now possible to see which parts-of-speech tags correspond to which counts. "
      ],
      "metadata": {
        "id": "Y4fGGbd-R_Z9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Store dictinoary with indexes and POS counts in a variable\n",
        "num_pos = doc.count_by(spacy.attrs.POS)\n",
        "\n",
        "dictionary = {}\n",
        "#Create a new dictionary which replaces the index of each part of speech for its label (NOUN, VERB, ADJECTIVE)\n",
        "for k,v in sorted(num_pos.items()):\n",
        "  dictionary[doc.vocab[k].text] = v\n",
        "\n",
        "dictionary"
      ],
      "metadata": {
        "id": "QGPdo6FtS4bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the same type of dictionary for each text in the DataFrame, a for loop can be created to nest the above process. The output will be a new DataFrame that includes the discipline of each paper and the frequency of each part of speech as appearing in that paper. If a paper does not contain a particular part-of-speech, the cell will read \"NaN\" (Not a Number). "
      ],
      "metadata": {
        "id": "0tRMrZWmSySc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get part of speech tags\n",
        "num_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(enriched_df.Text_NoStops.astype('unicode').values, batch_size=100):\n",
        "    dictionary = {}\n",
        "    num_pos = doc.count_by(spacy.attrs.POS)\n",
        "    for k,v in sorted(num_pos.items()):\n",
        "      dictionary[doc.vocab[k].text] = v\n",
        "    num_list.append(dictionary)\n",
        "\n",
        "pos_counts = pd.DataFrame(num_list)\n",
        "columns = list(pos_counts.columns)\n",
        "\n",
        "idx = 0\n",
        "new_col = pos_analysis_df['DISCIPLINE']\n",
        "pos_counts.insert(loc=idx, column='DISCIPLINE', value=new_col)\n",
        "\n",
        "pos_counts.head()"
      ],
      "metadata": {
        "id": "svVdo1ThJ05H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, it's simple to calculate the average number of each part of speech tag that appears in texts per genre by combining the ```groupby``` and ```mean``` functions. The output will be another DataFrame which shows the average part-of-speech use for each category in English and Biology."
      ],
      "metadata": {
        "id": "E6uwZGKiPZ5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_pos_df = pos_counts.groupby(['DISCIPLINE']).mean()\n",
        "average_pos_df = average_pos_df.round(2)\n",
        "average_pos_df = average_pos_df.reset_index()\n",
        "average_pos_df"
      ],
      "metadata": {
        "id": "rbK1VH5lZ7T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can examine the differences between parts of speech usage per genre. As suspected, it looks like Biology student papers use slightly more adjectives (17), on average, than English student papers, while an even greater number English papers (48), on average, use more verbs. Another interesting contrast is in the \"NUM\" tag: almost 50 more tokens, on average, are identified as a numeral in Biology papers than in English ones. Given the conventions of scientific research, this too makes sense; studies are much more frequently quantitative, incorporating lab measurements and statistical calculations. A bar graph can make some of these differences more apparent."
      ],
      "metadata": {
        "id": "mInEQKoUfvJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Use plotly to plot proper noun use per genre\n",
        "fig = px.bar(average_pos_df, x=\"DISCIPLINE\", y=[\"ADJ\", 'VERB', \"NUM\"], title=\"Average Part-of-Speech Usage Differences in Biology and English Student Writing\", barmode='group')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "F0NVs2tojM9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Though admittedly a simple analysis, calculating part-of-speech frequency counts affirms prior studies which posit a correlation between lexico-grammatical features and disciplinary conventions [(Hardy and Römer (2013)](https://web.p.ebscohost.com/ehost/pdfviewer/pdfviewer?vid=0&sid=df763712-4f88-480f-a421-987bb35a09cd%40redis) and indicates an application of spaCy that can be adapted to serve other researchers' corpora and part-of-speech usage queries. spaCy also provides a \"fine-grained\" tag set that could aid further research--for example, looking at how Biology and English students use sub-groups of verbs with different frequencies. Fine-grain tagging can be deployed in a similar loop to those above; just instead of retrieving the ```token.pos_``` for each word, call spacy to retrieve the ```token.tag_```:"
      ],
      "metadata": {
        "id": "FH8RDsonWtei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get part of speech tags\n",
        "tag_num_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(enriched_df.Text_NoStops.astype('unicode').values, batch_size=100):\n",
        "    tag_dictionary = {}\n",
        "    for token in doc: \n",
        "      num_tag = doc.count_by(spacy.attrs.TAG)\n",
        "      for k,v in sorted(num_tag.items()):\n",
        "        dictionary[doc.vocab[k].text] = v\n",
        "      tag_num_list.append(tag_dictionary)\n",
        "\n",
        "tag_counts = pd.DataFrame(num_list)\n",
        "columns = list(tag_counts.columns)\n",
        "\n",
        "idx = 0\n",
        "new_col = pos_analysis_df['DISCIPLINE']\n",
        "tag_counts.insert(loc=idx, column='DISCIPLINE', value=new_col)\n",
        "tag_counts"
      ],
      "metadata": {
        "id": "KO4hSnXxZdqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, the same type of frequency analysis can be performed on the fine-grained tags. P \n",
        "\n",
        "The example here is only one of many possible applications for part-of-speech tagging. art-of-speech tagging is also useful for research questions about sentence [*intent*](https://nostarch.com/download/samples/NLP_Vasiliev_ch2.pdf); the meaning of a text changes depending on whether the past, present, or infinitive form of a particular verb is used. It's valuable for word sense disambiguation and language translation. And of course, part-of-speech tagging is a building block of named entity recogntion, the focus of the analysis below.  "
      ],
      "metadata": {
        "id": "ocYLtmF_doQB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Named Entity Analysis"
      ],
      "metadata": {
        "id": "xC6eZIpIyb8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Research Question:** Do students use certain *named entities* more frequently in different academic genres that signify differences in genre conventions?  \n",
        "\n",
        "As with disciplines, research has shown that different genres of writing have their own conventions and expectations [(Berkenkotter and Huckin, 1995)](https://psycnet.apa.org/record/1995-97008-000). For example, explanatory genres like research papers, proposals and reports focus on analysis and explanation, whereas argumentative and critique-driven papers are driven by evaluations and arguments ([Römer and O'Donnell, 2011](https://web.s.ebscohost.com/ehost/pdfviewer/pdfviewer?vid=0&sid=0b9af0f6-d23e-47ae-90dc-e1ea9fbe606a%40redis); [Aull and Lancaster (2016)](https://journals.sagepub.com/doi/epub/10.1177/0741088318819472).\n",
        "\n",
        "By deploying spaCy on the MICUSP texts, researchers can determine whether there are any significant differences between the named entity frequencies in papers within the seven different genres represented (Argumentative Essay, Creative Writing, Critique/Evaluation, Proposal, Report, Research Paper, and Response Paper). We may suspect that argumentative genres engage more with people or organizations--in effect, entities with or against which they are taking a stance. Conversely, perhaps dates and numbers are more prevalent in evidence-heavy genres, like research papers and proposals. We'll use spaCy to test these hypotheses. \n",
        "\n",
        "To start, we'll create a new DataFrame with the text filenames, genres, and named entity tags. "
      ],
      "metadata": {
        "id": "yQHybuAdfbFK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new DataFrame for analysis purposes\n",
        "ner_analysis_df = enriched_df[['Filename','PAPER TYPE', 'NER_Tags', 'NER_Words']]\n",
        "ner_analysis_df.head()"
      ],
      "metadata": {
        "id": "U_WJ6sFTifpA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the str.count method, we can get counts of a specific named entity used in each text. Let's get the counts of the named entities of interest here (PERSON, ORG, DATE, and CARDINAL (numbers) ) and add them as new columns of the DataFrame. "
      ],
      "metadata": {
        "id": "4Hm9VNhB4fW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the number of each type of entity in each paper\n",
        "person_counts = ner_analysis_df['NER_Tags'].str.count('PERSON')\n",
        "org_counts = ner_analysis_df['NER_Tags'].str.count('ORG')\n",
        "date_counts = ner_analysis_df['NER_Tags'].str.count('DATE')\n",
        "cardinal_counts = ner_analysis_df['NER_Tags'].str.count('CARDINAL')\n",
        "\n",
        "#Append proper noun counts to new DataFrame \n",
        "ner_counts_df = pd.DataFrame()\n",
        "ner_counts_df['Genre'] = ner_analysis_df[\"PAPER TYPE\"]\n",
        "ner_counts_df['PERSON_Counts'] = person_counts\n",
        "ner_counts_df['ORG_Counts'] = org_counts\n",
        "ner_counts_df['DATE_Counts'] = date_counts\n",
        "ner_counts_df['CARDINAL_Counts'] = cardinal_counts\n",
        "\n",
        "ner_counts_df.head()"
      ],
      "metadata": {
        "id": "2MgWXgOL4K2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, we can calculate the average usage of each named entity and plot across paper type.\n"
      ],
      "metadata": {
        "id": "QACl2QXG8DOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Calculate average usage of each named entity type\n",
        "average_ner_df = ner_counts_df.groupby(['Genre']).mean()\n",
        "average_ner_df = average_ner_df.round(2)\n",
        "average_ner_df = average_ner_df.reset_index()\n",
        "average_ner_df\n",
        "\n",
        "#Use plotly to plot proper noun use per genre\n",
        "fig = px.bar(average_ner_df, x=\"Genre\", y=[\"PERSON_Counts\", 'ORG_Counts', \"DATE_Counts\", 'CARDINAL_Counts'], title=\"Average Part-of-Speech Usage Differences in Biology and English Student Writing\", barmode='group')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "OXUlrCXhh_0f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As hypothesized, the most dates and numbers are used in description-heavy proposals and research papers, while more people are referenced in critiques and evaluations. Interestingly, organizations are most invoked in proposals. Considering that spaCy defines ORG entities as companies, organizations, and institutions, this may still make sense in the context of research proposals, which involve putting forward a research question and justifying a study. Students thus may be making references to prior scholarship and institutions which have significance to new study. To explore this phenomenon further, retrieve the words tagged as this entity in each document. "
      ],
      "metadata": {
        "id": "GnbRUBibmR4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Named Entitie words\n",
        "date_w_list = []\n",
        "\n",
        "for doc in nlp.pipe(enriched_df.Text_NoStops.astype('unicode').values, batch_size=100):\n",
        "  ent_list = []\n",
        "  for ent in doc.ents:\n",
        "    if ent.label_ == 'DATE':\n",
        "      ent_list.append(ent.text)\n",
        "  date_w_list.append(ent_list)\n",
        "\n",
        "ner_analysis_df['DATE_Words'] = date_w_list\n",
        "ner_analysis_df['DATE_Words'] = [', '.join(map(str, l)) for l in ner_analysis_df['DATE_Words']]\n",
        "\n",
        "\n",
        "#Check named entities\n",
        "ner_analysis_df.head()"
      ],
      "metadata": {
        "id": "MAYxgfQ4nnkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now retrieve only the subset of essays that are in the proposal genre and get the top words that have been tagged as \"DATE in these essays and append them to a list. The majority are standard 4-digit dates, as well as an instance of 'al'; though further analysis is certainly needed to confirm, these date entities seem to indicate citation references are occuring; this fits in with the expectations of the genre, which require references to prior scholarship to justify the student's proposed claim."
      ],
      "metadata": {
        "id": "5oap6QrWxHjA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Search for only date words in proposal genre essays \n",
        "date_word_counts_df = ner_analysis_df[(ner_analysis_df == 'Proposal').any(axis=1)]\n",
        "\n",
        "#Count the frequency of each word in these essays and append to list\n",
        "date_word_frequencies = date_word_counts_df.DATE_Words.str.split(expand=True).stack().value_counts()\n",
        "date_word_frequencies[:10]"
      ],
      "metadata": {
        "id": "ELHvnBtNvLSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's contrast this with the top \"DATE\" entities in Critique/Evaluation papers. "
      ],
      "metadata": {
        "id": "P0KcaSLEx6av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "date_word_counts_df = ner_analysis_df[(ner_analysis_df == 'Critique/Evaluation').any(axis=1)]\n",
        "date_word_frequencies = date_word_counts_df.DATE_Words.str.split(expand=True).stack().value_counts()\n",
        "date_word_frequencies[:10]"
      ],
      "metadata": {
        "id": "2iKKPP-swqe2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, only three of the top dates tagged are words, and the rest are noun references to relative dates or periods. This too may indicate genre conventions such as providing context and/or centering an argument in relative space and time. Future research could analyze chains of named entities (and parts of speech) to get a better understanding of how these features work together to indicate larger rhetorical efforts. "
      ],
      "metadata": {
        "id": "mf7TCIuWzAVA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Download Enriched Dataset\n",
        "\n",
        "To wrap up, download the DataFrame with the enriched versions of the text. "
      ],
      "metadata": {
        "id": "gNnM4Uum94uH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enriched_df.to_csv('enriched_dataset.csv') \n",
        "files.download('enriched_dataset.csv')"
      ],
      "metadata": {
        "id": "V1W8n-0G942Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusions\n",
        "Through this tutorial, we've gleaned more information about the grammatical makeup of a text corpus. Such information can be valuable to researchers who are seeking to understand differences between texts in their corpus - for example, *what types of named entities are most common across the corpus? How frequently are certain words used as nouns vs. objects within individual texts and corpora, and what may this reveal about the content or themes of the texts themselves?* \n",
        "\n",
        "SpaCy is also a helpful tool to explore texts without fully-formed research questions in mind; exploring linguistic annotations like those mentioned above can propel further questions and text-mining pipelines, like the following: \n",
        "*   [Getting Started with Topic Modeling and Mallet (Graham, Weingart, and Milligan, 2012)](https://programminghistorian.org/en/lessons/topic-modeling-and-mallet#what-is-topic-modeling-and-for-whom-is-this-useful) - Describes process of conducting topic modeling on a corpora; the SpaCy tutorial can serve as a preliminary step to clean and explore data to be used in topic modeling\n",
        "*   [Sentiment Analysis for Exploratory Data Analysis (Saldaña, 2018)](https://programminghistorian.org/en/lessons/sentiment-analysis#calculate-sentiment-for-a-paragraph) - Describes how to conduct sentiment analysis using NLTK; the SpaCy tutorial provides alternative methods of pre-processing and exploration of entities that may become relevant in sentiment analysis \n",
        "\n"
      ],
      "metadata": {
        "id": "MaZ_fHSCiawV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bibliography\n",
        "\n",
        "Aull, L. (2019). Linguistic Markers of Stance and Genre in Upper-Level Student Writing. Written Communication, 36(2), 267–295. https://doi.org/10.1177/0741088318819472\n",
        "\n",
        "Burns, H. (2021). Crowdsourced-Data Normalization with Python and Pandas. Programming Historian, 10. https://doi.org/10.46430/phen0093\n",
        "\n",
        "Dombrowski, Q., Gniady, T., & Kloster, D. (2019). Introduction to Jupyter Notebooks. Programming Historian, 8. https://doi.org/10.46430/phen0087\n",
        "\n",
        "Froehlich, H. (2015). Corpus Analysis with Antconc. Programming Historian, 4. https://doi.org/10.46430/phen0043\n",
        "\n",
        "Graham, S., Weingart, S., & Milligan, I. (2012). Getting Started with Topic Modeling and MALLET. Programming Historian, 1. https://doi.org/10.46430/phen0017\n",
        "\n",
        "Hardy, J. A., & Römer, U. (2013). Revealing disciplinary variation in student writing: A multi-dimensional analysis of the Michigan Corpus of Upper-level Student Papers (MICUSP). Corpora, 8(2), 183–207. https://doi.org/10.3366/cor.2013.0040\n",
        "\n",
        "Kim, S. (2018). ‘Two rules are at play when it comes to none ’: A corpus-based analysis of singular versus plural none: Most grammar books say that the number of the indefinite pronoun none depends on formality level; corpus findings show otherwise. English Today, 34(3), 50–56. https://doi.org/10.1017/S0266078417000554\n",
        "\n",
        "Martin, F. and  Johnson, M. (2015). More Efficient Topic Modelling Through a Noun Only Approach . In Proceedings of Australasian Language Technology Association Workshop, pages 111−115. https://aclanthology.org/U15-1013.pdf \n",
        "\n",
        "O’Donnell, M. B., & Römer, U. (2012). From student hard drive to web corpus (part 2): The annotation and online distribution of the Michigan Corpus of Upper-level Student Papers (MICUSP). Corpora, 7(1), 1–18. https://doi.org/10.3366/cor.2012.0015\n",
        "\n",
        "Römer, U., & O’Donnell, M. B. (2011). From student hard drive to web corpus (part 1): The design, compilation and genre classification of the Michigan Corpus of Upper-level Student Papers (MICUSP). Corpora, 6(2), 159–177. https://doi.org/10.3366/cor.2011.0011\n",
        "\n",
        "Saldaña, Z. W. (2018). Sentiment Analysis for Exploratory Data Analysis. Programming Historian, 7. https://doi.org/10.46430/phen0079\n",
        "\n",
        "Turkel, W. J., & Crymble, A. (2012). Counting Word Frequencies with Python. Programming Historian, 1. https://doi.org/10.46430/phen0003\n",
        "\n",
        "Turkel, W. J., & Crymble, A. (2012). Python Introduction and Installation. Programming Historian, 1. https://doi.org/10.46430/phen0009\n",
        "\n",
        "Wieringa, J. (2012). Intro to Beautiful Soup. Programming Historian, 1. https://doi.org/10.46430/phen0008\n"
      ],
      "metadata": {
        "id": "nuLGSfoUg8h-"
      }
    }
  ]
}