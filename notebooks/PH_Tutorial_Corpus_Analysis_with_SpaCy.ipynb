{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "PcJjGxNrHIFg",
        "XUGnN4BX4C4c"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Corpus-Analysis-with-SpaCy/blob/main/notebooks/PH_Tutorial_Corpus_Analysis_with_SpaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus Analysis with spaCy\n",
        "##Introduction\n",
        "\n",
        "This tutorial describes how to conduct cleaning and text analysis on a corpus of texts using spaCy. It will be of interest to researchers who want to prepare a corpus of texts for analysis and perform lemmatization, part-of-speech tagging, and named entity recognition to help answer their research questions. \n",
        "\n",
        "###Why Use spaCy for Corpus Analysis? \n",
        "\n",
        "spaCy is an industrial-strength library for natural language processing. One of its primary usages is to retrieve a variety of linguistic annotations from a text or corpus (e.g. lemmas, part of speech tags, named entities), so it's valuable for researchers who want to know more about their corpora at the lexico-grammatical level. \n",
        "\n",
        "While there are several Python libraries that can conduct similar text-mining tasks, spaCy has the following advantages: \n",
        "*   It's **fast and simple to set up and call the nlp pipeline**; no need to call a wide range of packages and functions for each individual task [(Data Incubator, 2021)](https://www.thedataincubator.com/blog/2016/04/27/nltk-vs-spacy-natural-language-processing-in-python/)\n",
        "*   It uses only the **\"latest and best\" algorithms** for text-processing tasks, so it's easy to run and kept up-to-date by the developers [(Malhotra, 2018)](https://medium.com/@akankshamalhotra24/introduction-to-libraries-of-nlp-in-python-nltk-vs-spacy-42d7b2f128f2)\n",
        "*   It **performs better on text-splitting tasks** than NLTK, since it constructs syntactic trees for each sentence it is called on [(Proxet)]((https://proxet.com/blog/spacy-vs-nltk-natural-language-processing-nlp-python-libraries/)\n",
        "\n",
        "###Before You Begin\n",
        "\n",
        "You should have some familiarity with Python or a similar coding platform. For a brief introduction or refresher, work through some of the *Programming Historian's* [introductory Python tutorials](https://programminghistorian.org/en/lessons/introduction-and-installation). You should also have basic knowledge of spreadsheet (csv) files, as this tutorial will primarily use data in a similar format called a [pandas](https://pandas.pydata.org/) DataFrame. [This lesson](https://programminghistorian.org/en/lessons/crowdsourced-data-normalization-with-pandas) provides an overview to creating and manipulating datasets using pandas.\n",
        "\n",
        "It is also recommended, though not required, that you have some background in methods of computational text mining. [This lesson](https://programminghistorian.org/en/lessons/corpus-analysis-with-antconc) shares tips for working with plain text files and outlines possibilities for exploring keywords and collocates in a corpora (though using a different tool). [This lesson](https://programminghistorian.org/en/lessons/counting-frequencies) describes the process of counting word frequencies, a practice this tutorial will adapt to count part of speech and named entity tags. \n",
        "\n",
        "Two versions of code are provided for this tutorial: one version to be run on Jupyter Notebook and one for Google Colaboratory. Details and setup instructions for each are as follows: \n",
        "*  **Jupyter Notebook** is an environment through which you can run Python on your local machine. Since it's local, it works offline, and you can set up dedicated environments for your projects in which you'll only need to install packages once. If you've used Python before, you likely already have Jupyter Notebook installed on your machine. [This tutorial](https://programminghistorian.org/en/lessons/jupyter-notebooks) covers the basics of setting up Jupyter Notebook using Anaconda.\n",
        "\n",
        "*  **Google Colaboratory** is a Google platform which allows you to run Python in a web browser. Access is free with a Google account and nothing needs to be installed to your local machine. If you're new to coding, aren't working with sensitive data, and aren't running processes with [slow runtime](https://www.techrepublic.com/article/google-colab-vs-jupyter-notebook/), Google Colab may be the [best option for you. [Here's a brief Colab tutorial from Google.](https://colab.research.google.com/)\n",
        "\n",
        "\n",
        "###Lesson Dataset: Michigan Corpus of Upper-Level Student Papers (MICUSP)\n",
        "The [Michigan Corpus of Upper-Level Student Papers (MICUSP)](https://elicorpora.info/main) is a corpus of 829 high-scoring academic writing samples from students at the University of Michigan. The papers come from 16 disciplines and seven genres; all were written by senior undergraduate or graduate students and received an A-range score in a university course ([Römer and O'Donnell, 2011](https://web.s.ebscohost.com/ehost/pdfviewer/pdfviewer?vid=0&sid=0b9af0f6-d23e-47ae-90dc-e1ea9fbe606a%40redis); [O'Donnell and Römer, 2012](https://www.euppublishing.com/doi/10.3366/cor.2012.0015)). The papers and their metadata are publically available on MICUSP Simple, an online interface which allows users to search for papers by a range of fields (e.g. genre, discipline, student level, textual features) and conduct simple keyword analyses across disciplines and genres. Metadata from the corpus is available to download in csv form. The text files can be retrieved via webscraping, a process explained further in [this tutorial](https://programminghistorian.org/en/lessons/retired/intro-to-beautiful-soup).\n",
        "\n",
        "Given its size and robust metadata, MICUSP has become a valuable tool for researchers seeking to study student writing computationally. Notably, [Hardy and Römer (2013)](https://web.p.ebscohost.com/ehost/pdfviewer/pdfviewer?vid=0&sid=df763712-4f88-480f-a421-987bb35a09cd%40redis) use MICUSP to study language features that indicate how student writing differs across disciplines, [Aull and Lancaster (2016)](https://journals.sagepub.com/doi/epub/10.1177/0741088318819472) compare usages of stance markers across student genres, and [Kim (2018)](https://www.cambridge.org/core/product/identifier/S0266078417000554/type/journal_article) highlights discrepancies between prescriptive grammar rules and actual language use in student work. Though different and framework and approach, these studies are predicated on the fact that computational analysis of *language patterns*--the discrete lexico-grammatical practices students employ in their writing--can yield insights into larger questions about academic writing. Given its value in retrieving *linguistic annotations* like parts of speech and named entities, spaCy is well-poised to conduct this type of analysis using MICUSP.\n",
        "\n",
        "For the purposes of this tutorial, we'll use at a subsection of MICUSP: 67 Biology papers and 98 English papers. Papers in this select corpus belong to all seven MICUSP genres: Argumentative Essay, Creative Writing, Critique/Evaluation, Proposal, Report, Research Paper, and Response Paper. This select corpus and the associated metadata csv are available to download as part of this tutorial's [lesson materials](https://github.com/mkane968/Corpus-Analysis-with-SpaCy/tree/main/lesson-materials). This tutorial will demonstrate how spaCy's utilities in **stopword removal,** **tokenization,** and **lemmatization,** can clean and prepare a corpus of student texts for analysis. It will also demonstrate how spaCy's ability to extract linguistic annotations like part-of-speech tags and named entities can be used to compare conventions within subsets of a discourse community of interest. Here, the focus will be on lexico-grammatical features that indicate genre and disciplinary differences in academic writing: \n",
        "*   *Genre Analysis:* Do students use certain **parts of speech** more frequently in some genres than others? And what can these differences tell us about genre conventions? For example, the goal of a proposal is to put forward a research proposal or question; it's more focused on \"big ideas\" than something like a response paper that narrowly addresses a prior text ([Römer and O'Donnell, 2011](https://web.s.ebscohost.com/ehost/pdfviewer/pdfviewer?vid=0&sid=0b9af0f6-d23e-47ae-90dc-e1ea9fbe606a%40redis)). Does this translate to the linguistic level--do students use more proper nouns, for example, when writing in genres with broader goals? \n",
        "\n",
        "*   *Discipline Analysis:* Do students use certain **named entities** more frequently in Biology papers than in English papers? And what can these differences tell us about genre conventions? For example, even when writing in the same genres, the writer of a scientific research paper often has very different expectations than one in the humanities (Berkenkotter and Huckin, 1995).Does this translate to the linguistic level--do students use more concrete dates, and organization names in biology research papers than in English ones? \n",
        "\n",
        "Finally, this tutorial will address how a dataset enriched by spaCy can be exported in a usable format for further analyses like [Term Frequency - Inverse Document Frequency (tf-idf) analysis](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf), [sentiment analysis](https://programminghistorian.org/en/lessons/sentiment-analysis#calculate-sentiment-for-a-paragraph) or [topic modeling](https://programminghistorian.org/en/lessons/topic-modeling-and-mallet).\n",
        "\n",
        "###Tutorial Goals\n",
        "By the end of this tutorial, you will be able to: \n",
        "*   Upload a corpus of texts to Google Colab\n",
        "*   Clean the corpus by lowercasing, removing stop words and removing punctuation \n",
        "*   Enrich the corpus through lemmatization, chunking,  part-of-speech tagging, and named entity recognition\n",
        "*   Conduct frequency analyses with part-of-speech tags and named entities \n",
        "*   Download an enriched dataset for use in future NLP/analyses\n",
        "\n",
        "###Table of Contents: \n",
        "1. Install and Import Packages \n",
        "2. Load Text Files into DataFrame\n",
        "3. Cleaning and Tokenization\n",
        "4. Text Enrichment\n",
        "    \n",
        "    a. Lemmatization\n",
        "\n",
        "    b. Part of Speech Tagging\n",
        "\n",
        "    c. Named Entity Recognition\n",
        "\n",
        "5. Analysis of Linguistic Annotations\n",
        "\n",
        "    a. Part of Speech Differences Between Genres: Proper Nouns \n",
        "\n",
        "    b. Named Entity Differences Between Disciplines: Dates and Organizations \n",
        "6. Download Enriched Dataset"
      ],
      "metadata": {
        "id": "4wnznsMOACSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Install and Import Packages\n",
        "\n",
        "Install and import spaCy and related libraries and packages. It is common practice to do a single install at the very top of the file instead of interspersing them with your code to improve efficiency. These packages can be run in a single cell of code; below, the markdown text describes how each downloaded package or library will be used in the analysis. \n",
        "\n",
        "*Note: the first time you run this code, you may need to install spaCy itself; following this, you can just retrieve the associated packages using the import function*\n"
      ],
      "metadata": {
        "id": "wVeD4Ik7D43F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrm0CzvO_Uhw"
      },
      "outputs": [],
      "source": [
        "#Imports spaCy itself, necessary to use features \n",
        "#!pip install spaCy\n",
        "import spacy\n",
        "#Load the natural language processing pipeline\n",
        "#e'll choose eng_core_web_sm, the small English pipeline which has been tagged on written web texts\n",
        "#!python -m spacy download en_core_web_md \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#Load spaCy visualizer\n",
        "from spacy import displacy\n",
        "#Import drive and files to facilitate file uploads\n",
        "from google.colab import drive\n",
        "from google.colab import files\n",
        "#Import pandas DataFrame packages\n",
        "import pandas as pd\n",
        "#Import graphing package\n",
        "import plotly.graph_objects as go"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Load Text Files into DataFrame\n",
        "\n",
        "After all necessary packages have been installed, it is time to upload the texts for analysis. The key here is to read the texts into Google Colab in a way that will make them recognizable for analysis. Run the following code to “mount” the Google Drive, which allows your Google Colab notebook to access any files on your Drive. A box will pop up asking for permission for the notebook to access your Drive files; click “Connect to Google Drive,” select Google account to connect to, and click “Allow.” "
      ],
      "metadata": {
        "id": "zQ8ve667EvxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Oj5Ufz8xE7qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, load the files for analysis into your Google Drive. To complete this step, you must have the files of interest saved in a folder on your local machine. Once you run this line of code, a button will pop up directing you to “Choose Files” – click the button and a file explorer box will pop up. From here, navigate to the folder where you have stored the papers (.txt files), select the files of interest, and click “Open.” The files will then be uploaded to your Google Drive; you will see the upload complete as output of your cell and can access the files by clicking the file icon in the bar on the left-hand side of the notebook.\n"
      ],
      "metadata": {
        "id": "0FqwCyl8gtNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Selet multiple files to upload from local folder\n",
        "uploaded_files = files.upload()"
      ],
      "metadata": {
        "id": "XaVUPnFIE_kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have files upon which we can perform analysis. To check what form of data we are working with, use the type() function. It should return that your files are contained in a dictionary, where keys are the file names and values are the content of each file. \n"
      ],
      "metadata": {
        "id": "pg0w6jIEgxlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(uploaded_files)"
      ],
      "metadata": {
        "id": "N3f8cxLrgzUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we’ll make the data easier to manage by inserting it into a Pandas DataFrame. This will organize the texts into a table of rows and columns–in this case, the first column will contain the names of the files, and the second column will contain the context of each file. Since the files are currently stored in a dictionary, use the DataFrame.from_dict() function to append them to a new DataFrame.\n"
      ],
      "metadata": {
        "id": "fmYyZTzog32k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add files into DataFrame\n",
        "paper_df = pd.DataFrame.from_dict(uploaded_files, orient='index')\n",
        "paper_df.head()"
      ],
      "metadata": {
        "id": "s2w09XuhKqOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, you can reset the index (the very first row of the DataFrame). This will make data wrangling easier later.  "
      ],
      "metadata": {
        "id": "yr-yzcsng6o4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reset index and add column names to make wrangling easier\n",
        "paper_df = paper_df.reset_index()\n",
        "paper_df.columns = [\"Filename\", \"Text\"]\n",
        "paper_df.head()"
      ],
      "metadata": {
        "id": "BJJPgl5FL9qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we will add in metadata of interest to this data frame. Here, we'll add discipline and genre information connected to the MICUSP papers, as we'll be interested in using SpaCy to trace differences across genre and disciplinary categories later. When the cell is run, click \"choose files\", navigate to where you have stored the metadata.csv file, and select this file to puload."
      ],
      "metadata": {
        "id": "IinbZgtMcSOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload csv with essay metadata\n",
        "metadata = files.upload()"
      ],
      "metadata": {
        "id": "ZCASvLyJcq7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll convert the uploaded csv file to a second DataFrame, drop any empty columns and display the first five rows to check that the data is as expected. Four rows should be present: the paper IDs, their titles, their discipline, and their type."
      ],
      "metadata": {
        "id": "rjYwCN29i51g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_df = pd.read_csv('metadata.csv')\n",
        "metadata_df = metadata_df.dropna(axis=1, how='all')\n",
        "metadata_df.head()"
      ],
      "metadata": {
        "id": "gby6n4lzcq-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the paper ids in this DataFrame are *almost* the same as the paper file names. We're going to make them match exactly so we can merge the two DataFrames together on this column; in effect, linking each text with their title, discipline and genre. \n",
        "\n",
        "To match the columns, we'll remove the \".txt\" tag from the end of each filename in the paper DataFrame, and then we'll rename the paper id column as \"Filename.\""
      ],
      "metadata": {
        "id": "99vmOHTKezUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove .txt from titleo f each paper\n",
        "paper_df['Filename'] = paper_df['Filename'] .map(lambda x: x.rstrip('.txt'))\n",
        "\n",
        "#Rename column from paper ID to Title\n",
        "metadata_df.rename(columns={\"PAPER ID\": \"Filename\"}, inplace=True)"
      ],
      "metadata": {
        "id": "RO4lwuwJcrID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is possible to combine the papers and metadata into a single DataFrame. Check the first five rows to make sure each has a filaneme, title, discipline, paper type and text (the full paper)."
      ],
      "metadata": {
        "id": "TL-Wtoy6gVc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge metadata and papers into new DataFrame\n",
        "#Will only keep rows where both essay and metadata are present\n",
        "final_paper_df = metadata_df.merge(paper_df,on='Filename')\n",
        "\n",
        "#Print DataFrame\n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "2eCYbDExeuqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The resulting DataFrame is now ready for cleaning and analysis. \n"
      ],
      "metadata": {
        "id": "IHOPW8N_g9B_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Cleaning and Tokenization\n",
        "\n",
        "From a quick scan of the DataFrame, it is evident that some preliminary cleaning is required. First use the .decode() module to remove any utf-8 characters embedded in the texts (b'\\xef\\xbb\\xbf). It is also important to remove newline characters (\\n, \\r) through a simple string replacement line. These are NOT functions of spaCy but are necessary to make the code recognizable for further cleaning and tokenization. \n"
      ],
      "metadata": {
        "id": "Il5slp5CMKeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
        "final_paper_df['Text'] = final_paper_df['Text'].apply(lambda x: x.decode('utf-8', errors='ignore'))\n",
        "final_paper_df.head()\n",
        "\n",
        "#Remove newline characters\n",
        "final_paper_df['Text'] = final_paper_df['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
        "final_paper_df['Text'] = final_paper_df['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "buepJxsC-wRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next, most basic operation to perform is lowercasing all tokens in the texts. This will prevent incorrect calculations in later case-sensitive analysis; for example, if lowercasing is not performed, “House” and “house” may be counted as two different words. "
      ],
      "metadata": {
        "id": "xxBqzHdehT7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lowercase all words\n",
        "final_paper_df['Text'] = final_paper_df['Text'].str.lower()\n",
        "\n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "s8iYmAYsENde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to remove punctuation. Depending on your analysis goals, you may want to keep punctuation, but in this case we are interested in words only. "
      ],
      "metadata": {
        "id": "hpu_RZP1IOeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove punctuation and replace with no space (except periods and hyphens)\n",
        "final_paper_df['Text'] = final_paper_df['Text'].str.replace(r'[^\\w\\-\\.\\'\\s]+', '', regex = True)\n",
        "\n",
        "#Remove periods and replace with space (to prevent incorrect compounds)\n",
        "final_paper_df['Text'] = final_paper_df['Text'].str.replace(r'[^\\w\\-\\'\\s]+', ' ', regex = True)\n",
        "\n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "Q0-dpw9yIXY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process used to split up full text into smaller parts for analysis. SpaCy has a built-in function for tokenization that involves segmenting texts into individual parts like words and punctuation. Take the example of an individual sentence: "
      ],
      "metadata": {
        "id": "CIZWFJ5thWZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"This is 'an' example? sentence\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "AYx6IjM_ZiU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this function is doing is calling the spaCy nlp pipeline, which contains the data and components needed for text processing. When the nlp pipeline is called on a sentence, it splits that sentence on each whitespace and reviews its components. Components are then split based on rules for words, punctuations, prefixes, suffixes, etc. Each token is then loaded into a new object that we’ve called “doc.” Calling nlp also enables part of speech tagging, lemmatization, and other enrichment procedures we’ll discuss further below. \n",
        "\n",
        "Since we are working with multiple long texts, we are going to use nlp.pipe, which processes batches of texts as doc objects. Here we’ll tokenize each text in our DataFrame, append each set of tokens to a list, and add the new token lists to a new column in the DataFrame.\n"
      ],
      "metadata": {
        "id": "p5LzNhX6ha97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize with spaCy\n",
        "#Create list for tokens\n",
        "token_list = []\n",
        "\n",
        "# Disable POS, Dependency Parser, and NER since all we want is tokenizer \n",
        "with nlp.disable_pipes('tagger', 'parser', 'ner'):\n",
        "  #Iterate through each doc object (each text in DataFrame) and tokenize, append tokens to list\n",
        "    for doc in nlp.pipe(final_paper_df.Text.astype('unicode').values, batch_size=100):\n",
        "        word_list = []\n",
        "        for token in doc:\n",
        "            word_list.append(token.text)\n",
        "\n",
        "        token_list.append(word_list)\n",
        "        \n",
        "#Make token list a new column in DataFrame\n",
        "final_paper_df['Text_Tokens'] = token_list\n",
        "final_paper_df['Text_Tokens'] = [' '.join(map(str, l)) for l in final_paper_df['Text_Tokens']]\n",
        "\n",
        "#Check token list\n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "av2FqJD5HAE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When tokenizing texts, you can also exclude stopwords. Stopwords are words which may hold little significance to text analysis, such as very common words like “the” or “and.” SpaCy has a built-in dictionary of stopwords which you can access. You can also add or remove your own stopwords, as shown below:"
      ],
      "metadata": {
        "id": "FUQF0upEhdm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding and removing stopwords to default list\n",
        "#See list of default stopwords\n",
        "print(nlp.Defaults.stop_words)\n",
        "\n",
        "#Remove a  stopword\n",
        "nlp.Defaults.stop_words.remove(\"becomes\")\n",
        "\n",
        "#Add stopword\n",
        "nlp.Defaults.stop_words.add(\"book\")\n",
        "\n",
        "#Check updated list of default stopwords\n",
        "print(nlp.Defaults.stop_words)"
      ],
      "metadata": {
        "id": "lBveRLWFSASN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tokenize texts without stopwords, follow the same process above using nlp.pipe, but only append tokens to list that are NOT included in stopwords list and append these to a new row in the DataFrame. \n"
      ],
      "metadata": {
        "id": "Il_t3zO-hflr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove all stopwords and append remaining tokens to new df column\n",
        "token_list_nostops = []\n",
        "\n",
        "# Disable POS, Dependency Parser, and NER since all we want is tokenizer \n",
        "with nlp.disable_pipes('tagger', 'parser', 'ner'):\n",
        "  #Iterate through each doc object (each text in DataFrame) and tokenize, append tokens to list\n",
        "    for doc in nlp.pipe(final_paper_df.Text.astype('unicode').values, batch_size=100):\n",
        "        nostops_word_list = []\n",
        "        for token in doc:\n",
        "            if token.text not in nlp.Defaults.stop_words:\n",
        "              nostops_word_list.append(token.text)\n",
        "\n",
        "        token_list_nostops.append(nostops_word_list)\n",
        "\n",
        "#Make token list a new column in DataFrame\n",
        "final_paper_df['Text_Tokens_NoStops'] = token_list_nostops\n",
        "final_paper_df['Text_Tokens_NoStops'] = [' '.join(map(str, l)) for l in final_paper_df['Text_Tokens_NoStops']]\n",
        "\n",
        "\n",
        "#Check list of tokens without stopwords\n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "8zyfrC5FS_bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the goals of your analysis, you may want to remove or keep stopwords. One case where stopword removal may be useful is if you want to compare document similarity. SpaCy calculates document similarity based on corpus word vectors; since stopwords are words that appear throughout texts, they will heighten document similarity scores even if their content is very different. To make this type of analysis more accurate, we'll load a larger spaCy pipeline (en_core_web_md). \n"
      ],
      "metadata": {
        "id": "HOGEZ1GUhiZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopwords Test Case - Word Vector Similarit\n",
        "#Load a larger pipeline with vectors to make stopword analysis more accurate\n",
        "!spacy download en_core_web_md\n",
        "nlp2 = spacy.load(\"en_core_web_md\")"
      ],
      "metadata": {
        "id": "gZF8YTdmelu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe the difference between these two texts, with and without stopwords. As expected, these two texts in different disciplines are less similar with stopwords removed."
      ],
      "metadata": {
        "id": "a6BYJH89nwKa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare similarity between two documents without stopwords\n",
        "doc1 = nlp2(final_paper_df.Text_Tokens[0])\n",
        "doc2 = nlp2(final_paper_df.Text_Tokens[72])\n",
        "print(f'The similarity between ' + str(final_paper_df.Filename[0]) + ' and ' + str(final_paper_df.Filename[100]) + ' with stopwords is ' + str(doc1.similarity(doc2)))\n",
        "\n",
        "# Compare similarity between two documents with stopwords\n",
        "doc1 = nlp2(final_paper_df.Text_Tokens_NoStops[0])\n",
        "doc2 = nlp2(final_paper_df.Text_Tokens_NoStops[72])\n",
        "print(f'The similarity between ' + str(final_paper_df.Filename[0]) + ' and ' + str(final_paper_df.Filename[100]) + ' without stopwords is ' + str(doc1.similarity(doc2)))\n"
      ],
      "metadata": {
        "id": "qXUK0X_Wnya7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopword removal is also useful for topic modeling and classification tasks, where finding general themes across documents is the goal. However, other types of analysis like sentiment analysis are highly sensitive and removing stopwords will change sentence meaning (e.g. removing “not” in the sentence “I was not happy”). When possible, it is recommended to run analysis with and without stopwords and see how the model is impacted. For the rest of this tutorial, we will be using the corpus without stopwords, but you are welcome to replicate analysis with them. \n",
        "\n",
        "At this point, we have a cleaned DataFrame on which we can use spaCy for further text enrichment. But if you don't need lemmas, parts of speech, and named entities for your further analytic purposes, you can download the DataFrame as it is to your machine using this code. "
      ],
      "metadata": {
        "id": "M9r1k5BXhtYu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_paper_df.to_csv('final_paper_df.csv') \n",
        "files.download('final_paper_df.csv')"
      ],
      "metadata": {
        "id": "9ZQa-iFvlffF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create a new DataFrame to which we'll add the spaCy enrichments (lemmas, part of speech tags, named entities). We'll only keep the filenames, discipline and paper type labels, and the tokens without stopwords. "
      ],
      "metadata": {
        "id": "5nIs6L_Hlfqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new DataFrame for text enrichment\n",
        "enriched_df = final_paper_df[['Filename','DISCIPLINE','PAPER TYPE','Text_Tokens_NoStops']].copy()\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "1xYi2qi3ibLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll also use the smaller spaCy pipeline which is optimized for tagging, parsing, lemmatizing and ner. It's not as accurate as the en_core_web_md (and _lg) models, but processing will be faster. [Learn more about the models here](https://spacy.io/models)."
      ],
      "metadata": {
        "id": "vUrcYvEqot4s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Text Enrichment"
      ],
      "metadata": {
        "id": "PcJjGxNrHIFg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Lemmatization\n",
        "\n",
        "SpaCy enables several types of text enrichment. We’ll start with lemmatization, which retrieves the dictionary root word of each word (e.g. “brighten” for “brightening”). Lemmatization is one of the functions that occurs when the nlp pipe is called; repeat the same process as above to iterate through each document in the dataframe and this time append all lemmas to new column. \n"
      ],
      "metadata": {
        "id": "lDBswM5fo6Bd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get lemmas\n",
        "lemma_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is lemmatization \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag lemma, append lemma to list\n",
        "  for doc in nlp.pipe(enriched_df.Text_Tokens_NoStops.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.lemma_)\n",
        "        \n",
        "    lemma_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in dataframe\n",
        "enriched_df['Lemmas'] = lemma_list\n",
        "enriched_df['Lemmas'] = [' '.join(map(str, l)) for l in enriched_df['Lemmas']]\n",
        "\n",
        "#Check lemmas\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "X7_NQzt-OXJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization can help reduce noise and refine results for researchers who are conducting keyword searches. For example, let's compare counts of the word \"write\" in the original token column and in the lemmatized column"
      ],
      "metadata": {
        "id": "dwV7mzEOkR5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'\"Write\" appears in the text tokens column ' + str(enriched_df.Text_Tokens_NoStops.str.count(\"write\").sum()) + ' times.')\n",
        "print(f'\"Write\" appears in the lemmas column ' + str(enriched_df.Lemmas.str.count(\"write\").sum()) + ' times.')"
      ],
      "metadata": {
        "id": "0PA6wMTkqWl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, there are more instances of \"write\" in the lemmas column, as the lemmatization process has grouped inflected word forms (writing, writer) into the base word \"write.\" For similar rasons, lemmatized data is also often used to [create topic models](https://towardsdatascience.com/topic-modelling-in-python-with-spacy-and-gensim-dc8f7748bdbf)."
      ],
      "metadata": {
        "id": "2I5QC-nnssGh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Part of Speech Tagging"
      ],
      "metadata": {
        "id": "zBUNn3BMkPQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nlp pipeline also enables the tagging of each word according to its part of speech. This code will append all parts of speech to a new DataFrame column. \n"
      ],
      "metadata": {
        "id": "FG9B5mpFh0-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get part of speech tags\n",
        "pos_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(enriched_df.Text_Tokens_NoStops.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.pos_)\n",
        "        \n",
        "    pos_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in DataFrame\n",
        "enriched_df['POS_Tags'] = pos_list\n",
        "enriched_df['POS_Tags'] = [' '.join(map(str, l)) for l in enriched_df['POS_Tags']]\n",
        "\n",
        "#Check pos tags\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "_kBAmxZK_0Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, you may want to get only a set of Part of Speech tags for further analysis--all of the proper nouns, for instance. "
      ],
      "metadata": {
        "id": "_90TFpWPuZ36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get part of speech tags\n",
        "pos_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(enriched_df.Text_Tokens_NoStops.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "      if token.pos_ == 'PROPN':\n",
        "        word_list.append(token)\n",
        "        \n",
        "    pos_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in DataFrame\n",
        "enriched_df['Proper_Nouns'] = pos_list\n",
        "enriched_df['Proper_Nouns'] = [', '.join(map(str, l)) for l in enriched_df['Proper_Nouns']]\n",
        "\n",
        "#Check pos tags\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "z5mtmwpxuaNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can do the same with larger clusters of parts of speech, like noun phrases; this is a proces called dependency parsing."
      ],
      "metadata": {
        "id": "1cv9_pdruivA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get part of speech tags\n",
        "np_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(enriched_df.Text_Tokens_NoStops.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for np in doc.noun_chunks:\n",
        "      word_list.append(np)\n",
        "    np_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in DataFrame\n",
        "enriched_df['Text_NounPhrases'] = np_list\n",
        "enriched_df['Text_NounPhrases'] = [', '.join(map(str, l)) for l in enriched_df['Text_NounPhrases']]\n",
        "\n",
        "#Check pos tags\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "RRaCFQc_ui4y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out the dictionary of SpaCy POS tags [here](https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/#:~:text=Spacy%20POS%20Tags%20List,-Every%20token%20is%20assigned%20a) and feel free to test out the process of retreiving different parts of speech using the code above. \n",
        "\n",
        "Closely related to POS tagging is dependency parsing, wherein SpaCy identifies how different segments of a text are related to each other. Once the grammatical structure of each sentence is identified, visualizations can be created to show the connections between different words. Since we are working with large texts, our code will break down each text into sentences (spans) and then create dependency visualizers for each span\n"
      ],
      "metadata": {
        "id": "-XOgbYCduPII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get dependency parsing for a single doc\n",
        "doc = nlp(enriched_df.Text_Tokens_NoStops[0]) \n",
        "print(doc)\n",
        "\n",
        "#Make each sentence a span to break up dependency visualizations\n",
        "spans = doc.sents\n",
        "\n",
        "#Create dependency visualizations \n",
        "displacy.render(spans, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "63qLzAWYuKiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Part of speech tagging is another way to reduce dimensionality of the data and identify how language is being used at the lexico-grammatical level. Some types of further analysis, like topic modeling, works better when only certain parts of speech, like nouns, are retained. And as we'll explore further below, parts of speech can be used to compare patterns of language use within subsets of a corpora (e.g. texts of different genres and disciplines)"
      ],
      "metadata": {
        "id": "B1aa-Xg3ySZp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### c. Named Entity Recognition"
      ],
      "metadata": {
        "id": "5L_tQTFV4HOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, SpaCy can tag “named entities” in your text, such as names, dates, organizations, and locations. We’ll again call the nlp pipeline on each document in the corpus and append the named entity tags to a new column. \n"
      ],
      "metadata": {
        "id": "39utqaRyh_M1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Named Entities\n",
        "ner_list = []\n",
        "\n",
        "with nlp.disable_pipes('tagger', 'parser'):\n",
        "    for doc in nlp.pipe(enriched_df.Text_Tokens_NoStops.astype('unicode').values, batch_size=100):\n",
        "      ent_list = []\n",
        "      for ent in doc.ents:\n",
        "        ent_list.append(ent.label_)\n",
        "      ner_list.append(ent_list)\n",
        " \n",
        "enriched_df['NER_Tags'] = ner_list\n",
        "enriched_df['NER_Tags'] = [' '.join(map(str, l)) for l in enriched_df['NER_Tags']]\n",
        "\n",
        "\n",
        "#Check named entities\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "3tlTiFcfJMbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can append another column with the words and phrases identified as named entities."
      ],
      "metadata": {
        "id": "jrV6_VSCydsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Named Entitie words\n",
        "ent_list = []\n",
        "\n",
        "with nlp.disable_pipes('tagger', 'parser'):\n",
        "    for doc in nlp.pipe(enriched_df.Text_Tokens_NoStops.astype('unicode').values, batch_size=100):\n",
        "        ent_list.append(doc.ents)\n",
        "\n",
        "enriched_df['NER_Texts'] = ent_list\n",
        "enriched_df['NER_Texts'] = [' '.join(map(str, l)) for l in enriched_df['NER_Texts']]\n",
        "\n",
        "\n",
        "#Check named entities\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "bO86j0szydz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy also allows you to visualize named entities within single texts, as follows: "
      ],
      "metadata": {
        "id": "fq0WXwUlyiQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get named entities in a single document and visualize\n",
        "doc = nlp(enriched_df.Text_Tokens_NoStops[0]) \n",
        "\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "ldCTllIVyiWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Named entity recognition allows researchers to take a closer look at the \"real-world objects\" that are present in their texts: names, persons, institutions, events, works of art, and [more](https://www.kaggle.com/code/curiousprogrammer/entity-extraction-and-classification-using-spacy). In addition to studying named entities that spaCy automatically recognizees, you can use a training dataset to update the categories or create a new entity category, as in [this example](machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/)."
      ],
      "metadata": {
        "id": "indhranu0rSt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Analysis of Linguistic Annotations"
      ],
      "metadata": {
        "id": "XUGnN4BX4C4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new DataFrame for analysis purposes\n",
        "analysis_df = enriched_df[['Filename','DISCIPLINE', 'PAPER TYPE', 'POS_Tags', 'Proper_Nouns', 'NER_Tags','NER_Texts']]\n",
        "analysis_df.head()"
      ],
      "metadata": {
        "id": "ASdgE66V5q3Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### a. Part of Speech Analysis"
      ],
      "metadata": {
        "id": "9QTJrt6byTlf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One basic form of analysis is to count usages of specific parts of speech."
      ],
      "metadata": {
        "id": "ppZggNKRmeei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the number of proper nouns in each paper\n",
        "noun_counts = analysis_df['POS_Tags'].str.count('PROPN')\n",
        "\n",
        "#Append proper noun counts to DataFrame \n",
        "analysis_df['Proper_Noun_Counts'] = noun_counts\n",
        "analysis_df.head()"
      ],
      "metadata": {
        "id": "FuST5Z2Uk3Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, we can calculate the average usage of that part of speech and plot across paper type.\n",
        "\n"
      ],
      "metadata": {
        "id": "eqOw-fydpgay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get average of noun counts in each genre\n",
        "genre_mean_df = analysis_df.groupby('PAPER TYPE', as_index=False)['Proper_Noun_Counts'].mean()\n",
        "\n",
        "#Plot average proper noun counts by genre\n",
        "#Create bar graph\n",
        "#https://plotly.com/python/bar-charts/\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Proper Noun Counts', x=genre_mean_df['PAPER TYPE'], y=genre_mean_df['Proper_Noun_Counts']),\n",
        "])\n",
        "\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Counts of Proper Nouns in Each Genre')\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "V_zaCKvyk3ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "_azTltyDvt8U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### b. Named Entity Analysis"
      ],
      "metadata": {
        "id": "xC6eZIpIyb8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to part of speech analysis, we can get counts of a specific named entity"
      ],
      "metadata": {
        "id": "4Hm9VNhB4fW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the number of dates in each paper\n",
        "date_counts = analysis_df['NER_Tags'].str.count('DATE')\n",
        "\n",
        "#Append proper noun counts to DataFrame \n",
        "analysis_df['DATE_Counts'] = date_counts\n",
        "analysis_df.head()"
      ],
      "metadata": {
        "id": "2MgWXgOL4K2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, we can calculate the average usage of that named entity and plot across discipline and paper type.\n"
      ],
      "metadata": {
        "id": "QACl2QXG8DOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get average of noun counts in each discipline\n",
        "discipline_mean_df = analysis_df.groupby('DISCIPLINE', as_index=False)['DATE_Counts'].mean()\n",
        "\n",
        "#Create bar graph and plot proper noun count averages\n",
        "#https://plotly.com/python/bar-charts/\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Date Counts', x=discipline_mean_df['DISCIPLINE'], y=discipline_mean_df['DATE_Counts']),\n",
        "])\n",
        "\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Counts of Dates in Each Discipline')\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "6M3uMVAW4K5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can do the same thing in counting the average mentions of organization in each discipline."
      ],
      "metadata": {
        "id": "0VVFk0CL7dVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the number of dates in each paper\n",
        "person_counts = analysis_df['NER_Tags'].str.count('PERSON')\n",
        "\n",
        "#Append proper noun counts to DataFrame \n",
        "analysis_df['PERSON_Counts'] = person_counts\n",
        "\n",
        "#Get average of noun counts in each discipline\n",
        "discipline_mean_df2 = analysis_df.groupby('DISCIPLINE', as_index=False)['PERSON_Counts'].mean()\n",
        "\n",
        "#Create bar graph and plot proper noun count averages\n",
        "#https://plotly.com/python/bar-charts/\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Person Counts', x=discipline_mean_df2['DISCIPLINE'], y=discipline_mean_df2['PERSON_Counts']),\n",
        "    go.Bar(name='Date Counts', x=discipline_mean_df['DISCIPLINE'], y=discipline_mean_df['DATE_Counts']),\n",
        "\n",
        "])\n",
        "\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Counts of Dates and Person Entities in Each Discipline')\n",
        "fig.update_layout(barmode='group')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "vs3zsZI77Zct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6. Download Enriched Dataset\n",
        "\n",
        "To wrap up, download the DataFrame with the enriched versions of the text. "
      ],
      "metadata": {
        "id": "gNnM4Uum94uH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "enriched_df.to_csv('enriched_dataset.csv') \n",
        "files.download('enriched_dataset.csv')"
      ],
      "metadata": {
        "id": "V1W8n-0G942Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusions\n",
        "Through this tutorial, we've gleaned more information about the grammatical makeup of a text corpus. Such information can be valuable to researchers who are seeking to understand differences between texts in their corpus - for example, *what types of named entities are most common across the corpus? How frequently are certain words used as nouns vs. objects within individual texts and corpora, and what may this reveal about the content or themes of the texts themselves?* \n",
        "\n",
        "SpaCy is also a helpful tool to explore texts without fully-formed research questions in mind; exploring linguistic annotations like those mentioned above can propel further questions and text-mining pipelines, like the following: \n",
        "*   [Getting Started with Topic Modeling and Mallet (Graham, Weingart, and Milligan, 2012)](https://programminghistorian.org/en/lessons/topic-modeling-and-mallet#what-is-topic-modeling-and-for-whom-is-this-useful) - Describes process of conducting topic modeling on a corpora; the SpaCy tutorial can serve as a preliminary step to clean and explore data to be used in topic modeling\n",
        "*   [Sentiment Analysis for Exploratory Data Analysis (Saldaña, 2018)](https://programminghistorian.org/en/lessons/sentiment-analysis#calculate-sentiment-for-a-paragraph) - Describes how to conduct sentiment analysis using NLTK; the SpaCy tutorial provides alternative methods of pre-processing and exploration of entities that may become relevant in sentiment analysis \n",
        "\n"
      ],
      "metadata": {
        "id": "MaZ_fHSCiawV"
      }
    }
  ]
}