{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "pJqY4HBZkKWr"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Corpus-Analysis-with-SpaCy/blob/main/DRAFT_2_Corpus_Analysis_with_SpaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus Analysis with spaCy\n",
        "##Introduction\n",
        "\n",
        "This tutorial describes how to conduct cleaning and text analysis on a corpus of texts using spaCy. It will be of interest to researchers who want to prepare a corpus of texts for analysis and perform lemmatization, part-of-speech tagging, and named entity recognition to help answer their research questions. \n",
        "\n",
        "###Why Use spaCy for Corpus Analysis? \n",
        "\n",
        "spaCy is an industrial-strength library for natural language processing. One of its primary usages is to retrieve a variety of linguistic annotations from a text or corpus (e.g. lemmas, part of speech tags, named entities), so it's valuable for researchers who want to know more about their corpora at the lexico-grammatical level. \n",
        "\n",
        "While there are several Python libraries that can conduct similar text-mining tasks, spaCy has the following advantages: \n",
        "*   It's **fast and simple to set up and call the nlp pipeline**; no need to call a wide range of packages and functions for each individual task [(Data Incubator, 2021)](https://www.thedataincubator.com/blog/2016/04/27/nltk-vs-spacy-natural-language-processing-in-python/)\n",
        "*   It uses only the **\"latest and best\" algorithms** for text-processing tasks, so it's easy to run and kept up-to-date by the developers [(Malhotra, 2018)](https://medium.com/@akankshamalhotra24/introduction-to-libraries-of-nlp-in-python-nltk-vs-spacy-42d7b2f128f2)\n",
        "*   It **performs better on text-splitting tasks** than NLTK, since it constructs syntactic trees for each sentence it is called on [(Proxet)]((https://proxet.com/blog/spacy-vs-nltk-natural-language-processing-nlp-python-libraries/)\n",
        "\n",
        "###Before You Begin\n",
        "\n",
        "You should have some familiarity with Python or a similar coding platform. For a brief introduction or refresher, work through some of the *Programming Historian's* [introductory Python tutorials](https://programminghistorian.org/en/lessons/introduction-and-installation). You should also have basic knowledge of spreadsheet (csv) files, as this tutorial will primarily use data in a similar format called a [pandas](https://pandas.pydata.org/) DataFrame. [This lesson](https://programminghistorian.org/en/lessons/crowdsourced-data-normalization-with-pandas) provides an overview to creating and manipulating datasets using pandas.\n",
        "\n",
        "It is also recommended, though not required, that you have some background in methods of computational text mining. [This lesson](https://programminghistorian.org/en/lessons/corpus-analysis-with-antconc) shares tips for working with plain text files and outlines possibilites for exploring keywords and collocates in a corpora (though using a different tool). [This lesson](https://programminghistorian.org/en/lessons/counting-frequencies) describes the process of counting word frequencies, a practice this tutorial will adapt to count part of speech and named entity tags. \n",
        "\n",
        "Two versions of code are provided for this tutorial: one version to be run on Jupyter Notebook and one for Google Colaboratory. Details and setup instructions for each are as follows: \n",
        "*  **Jupyter Notebook** is an environment through which you can run Python on your local machine. Since it's local, it works offline, and you can set up dedicated environments for your projects in which you'll only need to install packages once. If you've used Python before, you likely already have Jupyter Notebook installed on your machine. [This tutorial](https://programminghistorian.org/en/lessons/jupyter-notebooks) covers the basics of setting up Jupyter Notebook using Anaconda.\n",
        "\n",
        "*  **Google Colaboratory** is a Google platform which allows you to run Python in a web browser. Access is free with a Google account and nothing needs to be installed to your local machine. If you're new to coding, aren't working with sensitive data, and aren't running processes with [slow runtime](https://www.techrepublic.com/article/google-colab-vs-jupyter-notebook/), Google Colab may be the [best option for you. [Here's a brief Colab tutorial from Google.](https://colab.research.google.com/)\n",
        "\n",
        "\n",
        "###Lesson Dataset: Michigan Corpus of Upper-Level Student Papers (MICUSP)\n",
        "The [Michigan Corpus of Upper-Level Student Papers (MICUSP)](https://elicorpora.info/main) is a corpus of 829 high-scoring academic writing samples from students at the University of Michigan. The papers come from 16 disciplines and seven genres; all were written by senior undergraduate or graduate students and received an A-range score in a university course ([Römer and O'Donnell, 2011](https://web.s.ebscohost.com/ehost/pdfviewer/pdfviewer?vid=0&sid=0b9af0f6-d23e-47ae-90dc-e1ea9fbe606a%40redis); [O'Donnell and Römer, 2012](https://www.euppublishing.com/doi/10.3366/cor.2012.0015)). The papers and their metadata are publically available on MICUSP Simple, an online interface which allows users to search for papers by a range of fields (e.g. genre, discipline, student level, textual features) and conduct simple keyword analyses across disciplines and genres. Metadata from the corpus is available to download in csv form. The text files can be retrieved via webscraping, a process explained further in [this tutorial](https://programminghistorian.org/en/lessons/retired/intro-to-beautiful-soup).\n",
        "\n",
        "Given its size and robust metadata, MICUSP has become a valuable tool for researchers seeking to study student writing computationally. Notably, [Hardy and Römer (2013)](https://web.p.ebscohost.com/ehost/pdfviewer/pdfviewer?vid=0&sid=df763712-4f88-480f-a421-987bb35a09cd%40redis) use MICUSP to study language features that indicate how student writing differs across disciplines, [Aull and Lancaster (2016)](https://journals.sagepub.com/doi/epub/10.1177/0741088318819472) compare usages of stance markers across student genres, and [Kim (2018)](https://www.cambridge.org/core/product/identifier/S0266078417000554/type/journal_article) highlights discrepancies between prescriptive grammar rules and actual language use in student work. Though different and framework and approach, these studies are predicated on the fact that computational analysis of *language patterns*--the discrete lexico-grammatical practices students employ in their writing--can yield insights into larger questions about academic writing. Given its value in retrieving *linguitic annotations* like parts of speech and named entities, spaCy is well-poised to conduct this type of analysis using MICUSP.\n",
        "\n",
        "For the purposes of this tutorial, we'll use at a subsection of MICUSP: 67 Biology papers and 98 English papers. Papers in this select corpus belong to all seven MICUSP genres: Argumentative Essay, Creative Writing, Critique/Evaluation, Proposal, Report, Research Paper, and Response Paper. This select corpus and the associated metadata csv are available to download as part of this tutorial's [lesson materials](https://github.com/mkane968/Corpus-Analysis-with-SpaCy/tree/main/lesson-materials). This tutorial will demonstrate how spaCy's utilities in **stopword removal,** **tokenization,** and **lemmatization,** can clean and prepare a corpus of student texts for analysis. It will also demonstrate how spaCy's ability to extract linguistic annotations like part-of-speech tags and named entities can be used to compare conventions within subsets of a discourse community of interest. Here, the focus will be on lexico-grammatical features that indicate genre and disciplinary differences in academic writing: \n",
        "*   *Genre Analysis:* Do students use certain **parts of speech** more frequently in some genres than others? And what can these differences tell us about genre conventions? For example, the goal of a proposal is to put forward a research proposal or question; it's more focused on \"big ideas\" than something like a response paper that narrowly addresses a prior text ([Römer and O'Donnell, 2011](https://web.s.ebscohost.com/ehost/pdfviewer/pdfviewer?vid=0&sid=0b9af0f6-d23e-47ae-90dc-e1ea9fbe606a%40redis)). Does this translate to the linguistic level--do students use more proper nouns, for example, when writing in genres with broader goals? \n",
        "\n",
        "*   *Discipline Analysis:* Do students use certain **named entities** more frequently in Biology papers than in English papers? And what can these differences tell us about genre conventions? For example, even when writing in the same genres, the writer of a scientific research paper often has very different expectations than one in the humanities (Berkenkotter and Huckin, 1995).Does this translate to the linguistic level--do students use more concrete dates, and organization names in biology research papers than in English ones? \n",
        "\n",
        "Finally, this tutorial will address how a dataset enriched by spaCy can be exported in a useable format for further analyses like [Term Frequency - Inverse Document Frequency (tf-idf) analysis](https://programminghistorian.org/en/lessons/analyzing-documents-with-tfidf), [sentiment analysis](https://programminghistorian.org/en/lessons/sentiment-analysis#calculate-sentiment-for-a-paragraph) or [topic modeling](https://programminghistorian.org/en/lessons/topic-modeling-and-mallet).\n",
        "\n",
        "###Tutorial Goals\n",
        "By the end of this tutorial, you will be able to: \n",
        "*   Upload a corpus of texts to Google Colab\n",
        "*   Clean the corpus by lowercasing, removing stop words and removing punctuation \n",
        "*   Enrich the corpus through lemmatization, chunking,  part-of-speech tagging, and named entity recognition\n",
        "*   Conduct frequency analyses with part-of-speech tags and named entities \n",
        "*   Download an enriched dataset for use in future NLP/analyses\n",
        "\n",
        "###Table of Contents: \n",
        "1. Install Packages \n",
        "2. Load Text Files into DataFrame\n",
        "3. Cleaning and Tokenization\n",
        "4. Text Enrichment\n",
        "    \n",
        "    a. Lemmatization\n",
        "\n",
        "    b. Part of Speech Tagging\n",
        "\n",
        "    c. Parsing and Chunking\n",
        "    \n",
        "    d. Named Entity Recognition\n",
        "\n",
        "5. Analysis of Linguistic Annotations\n",
        "\n",
        "    a. Part of Speech Differences Between Genres: Proper Nouns \n",
        "\n",
        "    b. Named Entity Differences Between Disciplines: Dates and Organizations \n",
        "6. Download Enriched Dataset"
      ],
      "metadata": {
        "id": "4wnznsMOACSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Install Packages\n",
        "\n",
        "Import spaCy and related libraries and packages. It is common practice to do this at the very top of the file instead of interspersing them with your code to improve efficiency. These packages can be run in a single cell of code; below, the markdown text describes how each downloaded package or library will be used in the analysis. \n"
      ],
      "metadata": {
        "id": "wVeD4Ik7D43F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrm0CzvO_Uhw"
      },
      "outputs": [],
      "source": [
        "#Imports spaCy itself, necessary to use features \n",
        "#!pip install spaCy\n",
        "import spacy\n",
        "#Load the natural language processing pipeline\n",
        "#!python -m spacy download en_core_web_md \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#Load spaCy visualizer\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Load Text Files into DataFrame\n",
        "\n",
        "After all necessary packages have been installed, it is time to upload the texts for analysis. The key here is to read the texts into Google Colab in a way that will make them recognizable for analysis. Run the following code to “mount” the Google Drive, which allows your Google Colab notebook to access any files on your Drive. A box will pop up asking for permission for the notebook to access your Drive files; click “Connect to Google Drive,” select Google account to connect to, and click “Allow.” "
      ],
      "metadata": {
        "id": "zQ8ve667EvxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Oj5Ufz8xE7qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, load the files for analysis into your Google Drive. To complete this step, you must have the files of interest saved in a folder on your local machine. Once you run this line of code, a button will pop up directing you to “Choose Files” – click the button and a file explorer box will pop up. From here, navigate to the folder where your files are stored, select the files of interest, and click “Open.” The files will then be uploaded to your Google Drive; you will see the upload complete as output of your cell and can access the files by clicking the file icon in the bar on the left-hand side of the notebook.\n"
      ],
      "metadata": {
        "id": "0FqwCyl8gtNV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Selet multiple files to upload from local folder\n",
        "from google.colab import files\n",
        "\n",
        "uploaded_files = files.upload()"
      ],
      "metadata": {
        "id": "XaVUPnFIE_kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have files upon which we can perform analysis. To check what form of data we are working with, use the type() function. It should return that your files are contained in a dictionary, where keys are the file names and values are the content of each file. \n"
      ],
      "metadata": {
        "id": "pg0w6jIEgxlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(uploaded_files)"
      ],
      "metadata": {
        "id": "N3f8cxLrgzUq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we’ll make the data easier to manage by inserting it into a Pandas dataframe. This will organize the texts into a table of rows and columns–in this case, the first column will contain the names of the files, and the second column will contain the context of each file. Since the files are currently stored in a dictionary, use the DataFrame.from_dict() function to append them to a new dataframe.\n"
      ],
      "metadata": {
        "id": "fmYyZTzog32k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Add files into dataframe\n",
        "import pandas as pd\n",
        "\n",
        "paper_df = pd.DataFrame.from_dict(uploaded_files, orient='index')\n",
        "paper_df.head()"
      ],
      "metadata": {
        "id": "s2w09XuhKqOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, you can reset the index (the very first row of the dataframe). This will make data wrangling easier later.  "
      ],
      "metadata": {
        "id": "yr-yzcsng6o4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reset index and add column names to make wrangling easier\n",
        "paper_df = paper_df.reset_index()\n",
        "paper_df.columns = [\"Filename\", \"Text\"]\n",
        "paper_df.head()"
      ],
      "metadata": {
        "id": "BJJPgl5FL9qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optionally, we can add in metadata of interest to this data frame. Here, we'll add discipline and genre information, as we'll be interested in using SpaCy to trace differences across genre and disciplinary categories later. "
      ],
      "metadata": {
        "id": "IinbZgtMcSOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Upload csv with essay metadata\n",
        "metadata = files.upload()"
      ],
      "metadata": {
        "id": "ZCASvLyJcq7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metadata_df = pd.read_csv('metadata (2).csv')\n",
        "metadata_df = metadata_df.dropna(axis=1, how='all')\n",
        "metadata_df.head()"
      ],
      "metadata": {
        "id": "gby6n4lzcq-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need to do some minor editing so that the titles in the paper dataframe match the metadata paper ids. This is so that we can merge the two dataframes on the paper names. "
      ],
      "metadata": {
        "id": "99vmOHTKezUX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove .txt from titleo f each paper\n",
        "paper_df['Filename'] = paper_df['Filename'] .map(lambda x: x.rstrip('.txt'))\n",
        "\n",
        "#Rename column from paper ID to Title\n",
        "metadata_df.rename(columns={\"PAPER ID\": \"Filename\"}, inplace=True)"
      ],
      "metadata": {
        "id": "RO4lwuwJcrID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now it is possible to combine the papers and metadata into a single dataframe."
      ],
      "metadata": {
        "id": "TL-Wtoy6gVc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Merge metadata and papers into new dataframe\n",
        "#Will only keep rows where both essay and metadata are present\n",
        "final_paper_df = metadata_df.merge(paper_df,on='Filename')\n",
        "\n",
        "#Print dataframe\n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "2eCYbDExeuqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The texts in your resulting dataframe are now ready for cleaning and analysis. \n"
      ],
      "metadata": {
        "id": "IHOPW8N_g9B_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Cleaning and Tokenization\n",
        "\n",
        "From a quick scan of the dataframe, it is evident that some preliminary cleaning is required. First use the .decode() module to remove any utf-8 characters embedded in the texts (b'\\xef\\xbb\\xbf). It is also important to remove newline characters (\\n, \\r) through a simple string replacement line. These are NOT functions of spaCy but are necessary to make the code recognizable for further cleaning and tokenization. \n"
      ],
      "metadata": {
        "id": "Il5slp5CMKeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
        "final_paper_df['Text'] = final_paper_df['Text'].apply(lambda x: x.decode('utf-8', errors='ignore'))\n",
        "final_paper_df.head()\n",
        "\n",
        "#Remove newline characters\n",
        "final_paper_df['Text'] = final_paper_df['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
        "final_paper_df['Text'] = final_paper_df['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "buepJxsC-wRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next, most basic operation to perform is lowercasing all tokens in the texts. This will prevent incorrect calculations in later case-sensitive analysis; for example, if lowercasing is not performed, “House” and “house” may be counted as two different words. "
      ],
      "metadata": {
        "id": "xxBqzHdehT7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Lowercase all words\n",
        "final_paper_df['Text'] = final_paper_df['Text'].str.lower()\n",
        "\n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "s8iYmAYsENde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to remove punctuation. Depending on your analysis goals, you may want to keep punctuation, but in this case we are interested in words only. "
      ],
      "metadata": {
        "id": "hpu_RZP1IOeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove punctuation and replace with no space (except periods and hyphens)\n",
        "final_paper_df['Text'] = final_paper_df['Text'].str.replace(r'[^\\w\\-\\.\\'\\s]+', '', regex = True)\n",
        "\n",
        "#Remove periods and replace with space (to prevent incorrect compounds)\n",
        "final_paper_df['Text'] = final_paper_df['Text'].str.replace(r'[^\\w\\-\\'\\s]+', ' ', regex = True)\n",
        "\n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "Q0-dpw9yIXY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process used to split up full text into smaller parts for analysis. SpaCy has a built-in function for tokenization that involves segmenting texts into individual parts like words and punctuation. Take the example of an individual sentence: "
      ],
      "metadata": {
        "id": "CIZWFJ5thWZq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"This is 'an' example? sentence\")\n",
        "for token in doc:\n",
        "    print(token.text)"
      ],
      "metadata": {
        "id": "AYx6IjM_ZiU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What this function is doing is calling the nlp pipeline, which contains the data and components needed for text processing. When the nlp pipeline is called on a sentence, it splits that sentence on each whitespace and reviews its components. Components are then split based on rules for words, punctuations, prefixes, suffixes, etc. Each token is then loaded into a new object that we’ve called “doc.” Calling nlp also enables part of speech tagging, lemmatization, and other enrichment procedures we’ll discuss further below. \n",
        "\n",
        "Since we are working with multiple long texts, we are going to use nlp.pipe, which processes batches of texts as doc objects. Here we’ll tokenize each text in our dataframe, append each set of tokens to a list, and add the new token lists to a new column in the dataframe.\n"
      ],
      "metadata": {
        "id": "p5LzNhX6ha97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize with spaCy\n",
        "\n",
        "#Create list for tokens\n",
        "token_list = []\n",
        "\n",
        "# Disable POS, Dependency Parser, and NER since all we want is tokenizer \n",
        "with nlp.disable_pipes('tagger', 'parser', 'ner'):\n",
        "  #Iterate through each doc object (each text in dataframe) and tokenize, append tokens to list\n",
        "    for doc in nlp.pipe(final_paper_df.Text.astype('unicode').values, batch_size=100):\n",
        "        word_list = []\n",
        "        for token in doc:\n",
        "            word_list.append(token.text)\n",
        "\n",
        "        token_list.append(word_list)\n",
        "        \n",
        "#Make token list a new column in dataframe\n",
        "final_paper_df['Text_Tokens'] = token_list\n",
        "final_paper_df['Text_Tokens'] = [' '.join(map(str, l)) for l in final_paper_df['Text_Tokens']]\n",
        "\n",
        "#Check token list\n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "av2FqJD5HAE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When tokenizing texts, you can also exclude stopwords. Stopwords are words which may hold little significance to text analysis, such as very common words like “the” or “and.” SpaCy has a built-in dictionary of stopwords which you can access. You can also add or remove your own stopwords, as shown below:"
      ],
      "metadata": {
        "id": "FUQF0upEhdm3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding and removing stopwords to default list\n",
        "#See list of default stopwords\n",
        "print(nlp.Defaults.stop_words)\n",
        "\n",
        "#Remove a  stopword\n",
        "nlp.Defaults.stop_words.remove(\"becomes\")\n",
        "\n",
        "#Add stopword\n",
        "nlp.Defaults.stop_words.add(\"book\")\n",
        "\n",
        "#Check updated list of default stopwords\n",
        "print(nlp.Defaults.stop_words)"
      ],
      "metadata": {
        "id": "lBveRLWFSASN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To tokenize texts without stopwords, follow the same process above using nlp.pipe, but only append tokens to list that are NOT included in stopwords list and append these to a new row in the dataframe. \n"
      ],
      "metadata": {
        "id": "Il_t3zO-hflr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove all stopwords and append remaining tokens to new df column\n",
        "token_list_nostops = []\n",
        "\n",
        "# Disable POS, Dependency Parser, and NER since all we want is tokenizer \n",
        "with nlp.disable_pipes('tagger', 'parser', 'ner'):\n",
        "  #Iterate through each doc object (each text in dataframe) and tokenize, append tokens to list\n",
        "    for doc in nlp.pipe(final_paper_df.Text.astype('unicode').values, batch_size=100):\n",
        "        nostops_word_list = []\n",
        "        for token in doc:\n",
        "            if token.text not in nlp.Defaults.stop_words:\n",
        "              nostops_word_list.append(token.text)\n",
        "\n",
        "        token_list_nostops.append(nostops_word_list)\n",
        "\n",
        "#Make token list a new column in dataframe\n",
        "final_paper_df['Text_Tokens_NoStops'] = token_list_nostops\n",
        "final_paper_df['Text_Tokens_NoStops'] = [' '.join(map(str, l)) for l in final_paper_df['Text_Tokens_NoStops']]\n",
        "\n",
        "\n",
        "#Check list of tokens without stopwords\n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "8zyfrC5FS_bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Depending on the goals of your analysis, you may want to remove or keep stopwords. One case where stopword removal may be useful is if you want to compare document similarity. SpaCy calculates document similarity based on corpus word vectors; since stopwords are words that appear throughout texts, they will heighten document similarity scores even if their content is very different. Observe the difference here: \n"
      ],
      "metadata": {
        "id": "HOGEZ1GUhiZo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Stopwords Test Case - Word Vector Similarity\n",
        "#Load a larger pipeline with vectors\n",
        "#!spacy download en_core_web_md\n",
        "#nlp = spacy.load(\"en_core_web_md\")\n",
        "\n",
        "# Compare similarity between two documents without stopwords\n",
        "doc1 = nlp(final_paper_df.Text_Tokens[0])\n",
        "doc2 = nlp(final_paper_df.Text_Tokens[72])\n",
        "print(f'The similarity between ' + str(final_paper_df.Filename[0]) + ' and ' + str(final_paper_df.Filename[72]) + ' with stopwords is ' + str(doc1.similarity(doc2)))\n",
        "\n",
        "# Compare similarity between two documents without stopwords\n",
        "doc1 = nlp(final_paper_df.Text_Tokens_NoStops[0])\n",
        "doc2 = nlp(final_paper_df.Text_Tokens_NoStops[72])\n",
        "print(f'The similarity between ' + str(final_paper_df.Filename[0]) + ' and ' + str(final_paper_df.Filename[72]) + ' without stopwords is ' + str(doc1.similarity(doc2)))\n"
      ],
      "metadata": {
        "id": "gZF8YTdmelu8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopword removal is also useful for topic modeling and classification tasks, where finding general themes across documents is the goal. However, other types of analysis like sentiment analysis are highly sensitive and removing stopwords will change sentence meaning (e.g. removing “not” in the sentence “I was not happy”). When possible, it is recommended to run analysis with and without stopwords and see how the model is impacted. For the rest of this tutorial, we will be using the corpus without stopwords, but you are welcome to replicate analysis with them. \n"
      ],
      "metadata": {
        "id": "M9r1k5BXhtYu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Text Enrichment "
      ],
      "metadata": {
        "id": "PcJjGxNrHIFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Create new dataframe for text enrichment\n",
        "enriched_df = final_paper_df.copy()"
      ],
      "metadata": {
        "id": "1xYi2qi3ibLU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lemmatization"
      ],
      "metadata": {
        "id": "pJqY4HBZkKWr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy enables several types of text enrichment. We’ll start with lemmatization, which identifies the dictionary root word of each word (e.g. “brighten” for “brightening”). Lemmatization is one of the functions that occurs when the nlp pipe is called; repeat the same process as above to iterate through each document in the dataframe and this time append all lemmas to new column. \n"
      ],
      "metadata": {
        "id": "VuDhGegShusV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get lemmas\n",
        "lemma_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is lemmatization \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag lemma, append lemma to list\n",
        "  for doc in nlp.pipe(final_paper_df.Text_Tokens_NoStops.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.lemma_)\n",
        "        \n",
        "    lemma_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in dataframe\n",
        "final_paper_df['Text_Lemmas'] = lemma_list\n",
        "final_paper_df['Text_Lemmas'] = [' '.join(map(str, l)) for l in final_paper_df['Text_Lemmas']]\n",
        "\n",
        "#Check lemmas\n",
        "final_paper_df.head()"
      ],
      "metadata": {
        "id": "X7_NQzt-OXJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DISCUSS WHY LEMMATIZATION WOULD BE VALUABLE TO RESEARCHERS"
      ],
      "metadata": {
        "id": "dwV7mzEOkR5F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part of Speech Tagging"
      ],
      "metadata": {
        "id": "zBUNn3BMkPQc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The nlp pipeline also enables the tagging of each word according to its part of speech. This code will append all parts of speech to a new dataframe column. \n"
      ],
      "metadata": {
        "id": "FG9B5mpFh0-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get part of speech tags\n",
        "pos_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(enriched_df.Text_Tokens.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.pos_)\n",
        "        \n",
        "    pos_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in dataframe\n",
        "enriched_df['POS_Tags'] = pos_list\n",
        "enriched_df['POS_Tags'] = [' '.join(map(str, l)) for l in enriched_df['POS_Tags']]\n",
        "\n",
        "#Check pos tags\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "_kBAmxZK_0Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One basic form of analysis is to count usages of specific parts of speech."
      ],
      "metadata": {
        "id": "ppZggNKRmeei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the number of proper nouns in each paper\n",
        "noun_counts = enriched_df['POS_Tags'].str.count('PROPN')\n",
        "\n",
        "#Append proper noun counts to dataframe \n",
        "enriched_df['Noun_Counts'] = noun_counts\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "FuST5Z2Uk3Ha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, we can calculate the average usage of that part of speech and plot across discipline and paper type.\n",
        "\n"
      ],
      "metadata": {
        "id": "eqOw-fydpgay"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get average of noun counts in each discipline\n",
        "discipline_mean_df = enriched_df.groupby('DISCIPLINE', as_index=False)['Noun_Counts'].mean()\n",
        "\n",
        "#Create bar graph and plot proper noun count averages\n",
        "#https://plotly.com/python/bar-charts/\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Proper Noun Counts', x=discipline_mean_df['DISCIPLINE'], y=discipline_mean_df['Noun_Counts']),\n",
        "])\n",
        "\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Counts of Proper Nouns in Each Discipline')\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "TGLsl_0sk3Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get average of noun counts in each genre\n",
        "discipline_mean_df = enriched_df.groupby('PAPER TYPE', as_index=False)['Noun_Counts'].mean()\n",
        "\n",
        "#Plot average proper noun counts by genre\n",
        "#Create bar graph\n",
        "#https://plotly.com/python/bar-charts/\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Proper Noun Counts', x=discipline_mean_df['PAPER TYPE'], y=discipline_mean_df['Noun_Counts']),\n",
        "])\n",
        "\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Counts of Proper Nouns in Each Genre')\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "V_zaCKvyk3ZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TALK ABOUT APPLICATIONS OF THIS TYPE OF ANALYSIS--WHAT IT SIGNiFIES ABOUT GENERIC OR DISCIPLINARY DIFFERENCES, HOW IT CAN PROMPT FURTHER ANALYSIS\n",
        "\n",
        "LIKE ANALYSIS OF SPECIFIC WORDS BASED ON POS--ASSOCIATE THESE BELOW"
      ],
      "metadata": {
        "id": "BTj5UKpymyzl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, you may want to get only a set of Part of Speech tags for further analysis--all of the proper nouns, for instance. "
      ],
      "metadata": {
        "id": "qdUcpdCa_ZVl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get part of speech tags\n",
        "pos_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(enriched_df.Text_Tokens.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "      if token.pos_ == 'PROPN':\n",
        "        word_list.append(token)\n",
        "        \n",
        "    pos_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in dataframe\n",
        "enriched_df['Proper_Nouns'] = pos_list\n",
        "enriched_df['Proper_Nouns'] = [', '.join(map(str, l)) for l in enriched_df['Proper_Nouns']]\n",
        "\n",
        "#Check pos tags\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "lfPmWwGn_lZ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can do the same with larger phrases, like noun phrases"
      ],
      "metadata": {
        "id": "pZssV6n9Aepe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get part of speech tags\n",
        "np_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(enriched_df.Text_Tokens.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for np in doc.noun_chunks:\n",
        "      word_list.append(np)\n",
        "    np_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in dataframe\n",
        "enriched_df['Text_NounPhrases'] = np_list\n",
        "enriched_df['Text_NounPhrases'] = [', '.join(map(str, l)) for l in enriched_df['Text_NounPhrases']]\n",
        "\n",
        "#Check pos tags\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "T-N0ZDIaqbpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check out the dictionary of SpaCy POS tags [here.](https://machinelearningknowledge.ai/tutorial-on-spacy-part-of-speech-pos-tagging/#:~:text=Spacy%20POS%20Tags%20List,-Every%20token%20is%20assigned%20a) \n",
        "\n",
        "\n",
        "\n",
        "TALK ABOUT IDENTIFYING SPECIFIC NOUN CHUNKS"
      ],
      "metadata": {
        "id": "eAeUJXC6h36K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Closely related to POS tagging is dependency parsing, wherein SpaCy identifies how different segments of a text are related to each other. Once the grammatical structure of each sentence is identified, visualizations can be created to show the connections between different words. Since we are working with large texts, our code will break down each text into sentences (spans) and then create dependency visualizers for each span\n"
      ],
      "metadata": {
        "id": "IXV1g-ky91H5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get dependency parsing for single doc\n",
        "doc = nlp(enriched_df.Text_Tokens[0]) \n",
        "print(doc)\n",
        "\n",
        "#Make each sentence a span to break up dependency visualizations\n",
        "spans = doc.sents\n",
        "\n",
        "#Create dependency visualizations \n",
        "displacy.render(spans, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "aHZFtLvJCcU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, you may want to get only a set of Part of Speech tags for further analysis--all of the noun phrases, for instance. "
      ],
      "metadata": {
        "id": "SJkr8ES4Nf5m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get part of speech tags\n",
        "np_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(enriched_df.Text_Tokens.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for np in doc.noun_chunks:\n",
        "      word_list.append(np)\n",
        "    np_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in dataframe\n",
        "enriched_df['Text_NounPhrases'] = np_list\n",
        "enriched_df['Text_NounPhrases'] = [', '.join(map(str, l)) for l in enriched_df['Text_NounPhrases']]\n",
        "\n",
        "#Check pos tags\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "dG3AwYQXNgDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(np_list)"
      ],
      "metadata": {
        "id": "mR1vS1f1S734"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eVkXySieRV5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some types of further analysis, like topic modeling, works better when certain parts of speech are removed from the texts. This is another form of dimensionality reduction. "
      ],
      "metadata": {
        "id": "6clTZ_pQNsz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Named Entity Recognition"
      ],
      "metadata": {
        "id": "5L_tQTFV4HOT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, SpaCy can tag “named entities” in your text, such as names, dates, organizations, and locations. We’ll again call the nlp pipeline on each document in the corpus and append the named entities to a new column. \n"
      ],
      "metadata": {
        "id": "39utqaRyh_M1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Named Entities\n",
        "ner_list = []\n",
        "\n",
        "with nlp.disable_pipes('tagger', 'parser'):\n",
        "    for doc in nlp.pipe(enriched_df.Text_Tokens.astype('unicode').values, batch_size=100):\n",
        "      ent_list = []\n",
        "      for ent in doc.ents:\n",
        "        ent_list.append(ent.label_)\n",
        "      ner_list.append(ent_list)\n",
        " \n",
        "enriched_df['Text_NER'] = ner_list\n",
        "enriched_df['Text_NER'] = [' '.join(map(str, l)) for l in enriched_df['Text_NER']]\n",
        "\n",
        "\n",
        "#Check named entities\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "3tlTiFcfJMbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to part of speech analysis, we can get counts of a specific named entity"
      ],
      "metadata": {
        "id": "4Hm9VNhB4fW3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get the number of proper nouns in each paper\n",
        "noun_counts = enriched_df['Text_NER'].str.count('DATE')\n",
        "\n",
        "#Append proper noun counts to dataframe \n",
        "enriched_df['NE_Counts'] = noun_counts\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "2MgWXgOL4K2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, we can calculate the average usage of that named entity and plot across discipline and paper type.\n"
      ],
      "metadata": {
        "id": "QACl2QXG8DOs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get average of noun counts in each discipline\n",
        "discipline_mean_df = enriched_df.groupby('DISCIPLINE', as_index=False)['NE_Counts'].mean()\n",
        "\n",
        "#Create bar graph and plot proper noun count averages\n",
        "#https://plotly.com/python/bar-charts/\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Organization Counts', x=discipline_mean_df['DISCIPLINE'], y=discipline_mean_df['NE_Counts']),\n",
        "])\n",
        "\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Counts of Organizations in Each Discipline')\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "6M3uMVAW4K5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get average of noun counts in each genre\n",
        "discipline_mean_df = enriched_df.groupby('PAPER TYPE', as_index=False)['NE_Counts'].mean()\n",
        "\n",
        "#Plot average proper noun counts by genre\n",
        "#Create bar graph\n",
        "#https://plotly.com/python/bar-charts/\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "fig = go.Figure(data=[\n",
        "    go.Bar(name='Organization Counts', x=discipline_mean_df['PAPER TYPE'], y=discipline_mean_df['NE_Counts']),\n",
        "])\n",
        "\n",
        "# Change the bar mode\n",
        "fig.update_layout(title_text='Counts of Organizations in Each Genre')\n",
        "fig.update_layout(barmode='stack')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "4OsROoQV4oIz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TALK ABOUT APPLICATIONS OF THIS TYPE OF ANALYSIS--WHAT IT SIGNiFIES ABOUT GENERIC OR DISCIPLINARY DIFFERENCES, HOW IT CAN PROMPT FURTHER ANALYSIS\n",
        "\n",
        "LIKE ANALYSIS OF SPECIFIC WORDS BASED ON NER--ASSOCIATE THESE BELOW"
      ],
      "metadata": {
        "id": "QhtNIj-58Kgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Named Entitie words\n",
        "ent_list = []\n",
        "\n",
        "with nlp.disable_pipes('tagger', 'parser'):\n",
        "    for doc in nlp.pipe(enriched_df.Text_Tokens.astype('unicode').values, batch_size=100):\n",
        "        ent_list.append(doc.ents)\n",
        "\n",
        "enriched_df['Text_NER'] = ent_list\n",
        "enriched_df['Text_NER'] = [' '.join(map(str, l)) for l in enriched_df['Text_NER']]\n",
        "\n",
        "\n",
        "#Check named entities\n",
        "enriched_df.head()"
      ],
      "metadata": {
        "id": "awjD8rpS4oX8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SpaCy also allows you to visualize named entities within single texts, as follows: "
      ],
      "metadata": {
        "id": "xb5FdgvmiBsc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get named entities in a single document and visualize\n",
        "doc = nlp(new_df.Text_Tokens[0]) \n",
        "\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "kekdvrarS0F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Conclusions\n",
        "Through this tutorial, we've gleaned more information about the grammatical makeup of a text corpus. Such information can be valuable to researchers who are seeking to understand differences between texts in their corpus - for example, *what types of named entities are most common across the corpus? How frequently are certain words used as nouns vs. objects within individual texts and corpora, and what may this reveal about the content or themes of the texts themselves?* \n",
        "\n",
        "SpaCy is also a helpful tool to explore texts without fully-formed research questions in mind; exploring linguistic annotations like those mentioned above can propel further questions and text-mining pipelines, like the following: \n",
        "*   [Getting Started with Topic Modeling and Mallet (Graham, Weingart, and Milligan, 2012)](https://programminghistorian.org/en/lessons/topic-modeling-and-mallet#what-is-topic-modeling-and-for-whom-is-this-useful) - Describes process of conducting topic modeling on a corpora; the SpaCy tutorial can serve as a preliminary step to clean and explore data to be used in topic modeling\n",
        "*   [Sentiment Analysis for Exploratory Data Analysis (Saldaña, 2018)](https://programminghistorian.org/en/lessons/sentiment-analysis#calculate-sentiment-for-a-paragraph) - Describes how to conduct sentiment analysis using NLTK; the SpaCy tutorial provides alternative methods of pre-processing and exploration of entities that may become relevant in sentiment analysis \n",
        "\n"
      ],
      "metadata": {
        "id": "MaZ_fHSCiawV"
      }
    }
  ]
}