{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNSSE+lpQ6kx23e4d1ebgEm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mkane968/Corpus-Analysis-with-SpaCy/blob/main/Corpus_Analysis_with_SpaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Corpus Analysis with SpaCy\n",
        "##Introduction\n",
        "\n",
        "In this tutorial, you will learn how to conduct cleaning and text analysis on a corpus of texts using SpaCy. \n",
        "\n",
        "SpaCy is a popular open-source tool for natural language processing. It's particularly good at annotating linguistic data through part-of-speech tagging, chunking and named entity recognition, as well as calculating document similarity through word embeddings. SpaCy was built for production and has an integrated catalog of features. ([SpaCy 101](https://spacy.io/usage/spacy-101#pipelines))\n",
        "\n",
        "By the end of this tutorial, you will be able to: \n",
        "*   Upload a corpus of 2 or more texts to Google Colab\n",
        "*   Clean corpora by lowercasing, removing stop words and removing punctuation \n",
        "*   Enrich corpora with spaCy through stemming, lemmatization, chunking,  part-of-speech tagging, and named entity recognition. \n",
        "\n",
        "Table of Contents: \n",
        "1. Install Packages \n",
        "2. Load Text Files into DataFrame\n",
        "3. Cleaning and Tokenization\n",
        "4. Text Enrichment (Lemmatization, Part of Speech Tagging, Named Entity Recognition)"
      ],
      "metadata": {
        "id": "4wnznsMOACSy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##1. Install Packages"
      ],
      "metadata": {
        "id": "wVeD4Ik7D43F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xrm0CzvO_Uhw"
      },
      "outputs": [],
      "source": [
        "#Imports spaCy itself, necessary to use features \n",
        "#!pip install spaCy\n",
        "import spacy\n",
        "#Load the natural language processing pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#Load spaCy visualizer\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##2. Load Text Files into DataFrame\n",
        "\n"
      ],
      "metadata": {
        "id": "zQ8ve667EvxG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Oj5Ufz8xE7qQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Selet multiple files to upload from local folder\n",
        "from google.colab import files\n",
        "\n",
        "uploaded_files = files.upload()\n"
      ],
      "metadata": {
        "id": "XaVUPnFIE_kS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Add files into dataframe\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame.from_dict(uploaded_files, orient='index')\n",
        "df.head()"
      ],
      "metadata": {
        "id": "s2w09XuhKqOq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Reset index and add column names to make wrangling easier\n",
        "df = df.reset_index()\n",
        "df.columns = [\"Title\", \"Text\"]\n",
        "df"
      ],
      "metadata": {
        "id": "BJJPgl5FL9qY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3. Cleaning and Tokenization"
      ],
      "metadata": {
        "id": "Il5slp5CMKeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove encoding characters from Text column (b'\\xef\\xbb\\xbf)\n",
        "df['Text'] = df['Text'].apply(lambda x: x.decode('utf-8', errors='ignore'))\n",
        "df.head()\n",
        "\n",
        "#Remove newline characters\n",
        "df['Text'] = df['Text'].str.replace(r'\\s+|\\\\r', ' ', regex=True) \n",
        "df['Text'] = df['Text'].str.replace(r'\\s+|\\\\n', ' ', regex=True) \n",
        "df.head()"
      ],
      "metadata": {
        "id": "buepJxsC-wRg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Lowercase all words\n",
        "df['Text'] = df['Text'].str.lower()\n",
        "\n",
        "#Remove punctuation and replace with no space (except periods and hyphens)\n",
        "df['Text'] = df['Text'].str.replace(r'[^\\w\\-\\.\\'\\s]+', '', regex = True)\n",
        "\n",
        "#Remove periods and replace with space (to prevent incorrect compounds)\n",
        "df['Text'] = df['Text'].str.replace(r'[^\\w\\-\\'\\s]+', ' ', regex = True)\n",
        "df.head()\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "s8iYmAYsENde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize with spaCy\n",
        "\n",
        "#Create list for tokens\n",
        "token_list = []\n",
        "\n",
        "# Disable POS, Dependency Parser, and NER since all we want is tokenizer \n",
        "with nlp.disable_pipes('tagger', 'parser', 'ner'):\n",
        "  #Iterate through each doc object (each text in dataframe) and tokenize, append tokens to list\n",
        "    for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "        word_list = []\n",
        "        for token in doc:\n",
        "            word_list.append(token.text)\n",
        "\n",
        "        token_list.append(word_list)\n",
        "        \n",
        "#Make token list a new column in dataframe\n",
        "df['token_list'] = token_list\n",
        "\n",
        "#Check token list\n",
        "df.head()"
      ],
      "metadata": {
        "id": "av2FqJD5HAE_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Adding and removing stopwords to default list\n",
        "#See list of default stopwords\n",
        "print(nlp.Defaults.stop_words)\n",
        "\n",
        "#Remove a  stopword\n",
        "#nlp.Defaults.stop_words.remove(\"becomes\")\n",
        "\n",
        "#Add stopword\n",
        "#nlp.Defaults.stop_words.add(\"my_new_stopword\")\n",
        "\n",
        "#Check updated list of default stopwords\n",
        "print(nlp.Defaults.stop_words)"
      ],
      "metadata": {
        "id": "lBveRLWFSASN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Remove all stopwords and append remaining tokens to new df column\n",
        "token_list_nostops = []\n",
        "\n",
        "# Disable POS, Dependency Parser, and NER since all we want is tokenizer \n",
        "with nlp.disable_pipes('tagger', 'parser', 'ner'):\n",
        "  #Iterate through each doc object (each text in dataframe) and tokenize, append tokens to list\n",
        "    for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "        nostops_word_list = []\n",
        "        for token in doc:\n",
        "            if token.text not in nlp.Defaults.stop_words:\n",
        "              nostops_word_list.append(token.text)\n",
        "\n",
        "        token_list_nostops.append(nostops_word_list)\n",
        "\n",
        "#Make token list a new column in dataframe\n",
        "df['token_list_nostops'] = token_list_nostops\n",
        "\n",
        "#Check list of tokens without stopwords\n",
        "df.head()"
      ],
      "metadata": {
        "id": "8zyfrC5FS_bX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make stoptoken_list a string again\n",
        "df['Stop_Tokens'] = df['stoptoken_list'].str.join(' ')\n",
        "df"
      ],
      "metadata": {
        "id": "04mWqVlj046R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DISCUSS USE OF TExTS WITH/WITHOUT STOPWORDS"
      ],
      "metadata": {
        "id": "qxpwvLqGU_P0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Text Enrichment (Lemmatization, Part of Speech Tagging, Named Entity Recognition)"
      ],
      "metadata": {
        "id": "PcJjGxNrHIFg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Get lemmas\n",
        "lemma_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.lemma_)\n",
        "        \n",
        "    lemma_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in dataframe\n",
        "df['lemma_list'] = lemma_list\n",
        "\n",
        "#Check lemmas\n",
        "df.head()"
      ],
      "metadata": {
        "id": "X7_NQzt-OXJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get part of speech tags\n",
        "pos_list = []\n",
        "\n",
        "# Disable Dependency Parser, and NER since all we want is POS \n",
        "with nlp.disable_pipes('parser', 'ner'):\n",
        "  #Iterate through each doc object and tag POS, append POS to list\n",
        "  for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "    word_list = []\n",
        "    for token in doc:\n",
        "        word_list.append(token.pos_)\n",
        "        \n",
        "    pos_list.append(word_list)\n",
        "\n",
        "#Make pos list a new column in dataframe\n",
        "df['pos_list'] = pos_list\n",
        "\n",
        "#Check pos tags\n",
        "df.head()"
      ],
      "metadata": {
        "id": "_kBAmxZK_0Ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get dependency parsing for single doc\n",
        "doc = nlp(df.Text[0]) \n",
        "print(doc)\n",
        "\n",
        "#Make each sentence a span to break up dependency visualizations\n",
        "spans = doc.sents\n",
        "\n",
        "#Create dependency visualizations \n",
        "displacy.render(spans, style=\"dep\", jupyter=True)"
      ],
      "metadata": {
        "id": "aHZFtLvJCcU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get Named Entities\n",
        "ent_list = []\n",
        "\n",
        "with nlp.disable_pipes('tagger', 'parser'):\n",
        "    for doc in nlp.pipe(df.Text.astype('unicode').values, batch_size=100):\n",
        "        ent_list.append(doc.ents)\n",
        "\n",
        "df['ent_list'] = ent_list\n",
        "\n",
        "#Check named entities\n",
        "df.head()"
      ],
      "metadata": {
        "id": "3tlTiFcfJMbU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Get named entities in a single document and visualize\n",
        "doc = nlp(df.Text[0]) \n",
        "print(doc)\n",
        "\n",
        "displacy.render(doc, style=\"ent\", jupyter=True)"
      ],
      "metadata": {
        "id": "kekdvrarS0F6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}